[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2300 Intro to Data Science",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2300 - Intro to Data Science\nPrerequisites: None\n\nCourse Description:\nPrinciples of data science, including problem workflow, variable types, visualization, modeling, programming, data management and cleaning, reproducibility, and big data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Session 1 – R setup",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "01.html#objectives",
    "href": "01.html#objectives",
    "title": "1  Session 1 – R setup",
    "section": "",
    "text": "Explore the data‑science lifecycle. The R for Data Science introduction models data science as a cycle: you begin by importing and tidying raw data; then understand it through an iterative loop of transforming, visualising and modelling; and finally communicate your results. Programming surrounds all of these steps and supports them. Today you will learn why each component is important and how they connect.\nRecognise the tools you need. To run the code in R for Data Science you need R, RStudio, the tidyverse package collection and a handful of other packages. We will install R (from the Comprehensive R Archive Network), download RStudio (an integrated development environment), and install the tidyverse.\nPrepare your computing environment. By the end of class you should have R and RStudio installed, know how to install and load packages, and be able to run simple R commands (arithmetic, vector creation, summary statistics). We will also introduce a dataset that you will revisit throughout the semester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "01.html#notes",
    "href": "01.html#notes",
    "title": "1  Session 1 – R setup",
    "section": "Notes",
    "text": "Notes\n\nThe data‑science workflow\nData science is not a linear process. You first import data from files, databases or the web; tidy it so that each variable is a column and each observation a row; transform, visualise and model your data in an iterative loop to understand patterns and relationships; and then communicate your findings to others. Programming is a cross‑cutting skill that supports each of these phases. Throughout this course, you will move back and forth between these steps rather than following them in a strict order.\n\n\nTools and setup\nYou need four things to run the book’s code: R, RStudio, the tidyverse and some additional packages.\n\nR is the programming language you will use. Download the latest version from CRAN at https://cloud.r-project.org. A new major version of R is released once a year, with minor versions in between; updating regularly ensures compatibility.\n\nRStudio is an integrated development environment (IDE) for R. Download it from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year. When you start RStudio, you will see a console pane for typing R code and an output pane for plots.\nThe tidyverse is a collection of packages for data manipulation, visualization and programming. To install all core tidyverse packages at once, run install.packages(\"tidyverse\") in the R console. After installation, load the tidyverse with library(tidyverse); this attaches packages such as dplyr, ggplot2, tidyr, readr, stringr, forcats, lubridate, purrr and tibble. You only need to install a package once, but you must load it in each new R session.\nOther packages. We will occasionally use packages outside the tidyverse (e.g., palmerpenguins, nycflights13, arrow, rvest, duckdb). When you encounter an error that a package is not installed, run install.packages(\"package_name\") to install the package.\n\n\n\nInstalling and testing your environment\n\nInstall R and RStudio as described above. Accept the default installation options.\n\nInstall the tidyverse. Open RStudio and run the following in the console:\n\n\ninstall.packages(\"tidyverse\")   # installs core tidyverse packages\nlibrary(tidyverse)               \n\n\nTry basic R commands. Use R as a calculator and practise creating vectors and computing summaries:\n\n\n2 + 2                  # arithmetic\n\n[1] 4\n\nx &lt;- c(1, 2, 3, 5, 7)  # create a numeric vector\nx * 2                  # vectorised multiplication\n\n[1]  2  4  6 10 14\n\nmean(x)                # compute the average\n\n[1] 3.6\n\nsum(x &gt; 4)             # count values greater than 4 (logical vector)\n\n[1] 2\n\n\nNotice that R performs operations element‑wise on vectors, and the assignment operator &lt;- stores values. Use descriptive variable names and indent your code neatly; we will discuss code style in a later class.\n\n\nTake‑aways\n\nThe data‑science process is iterative, looping through import, tidy, transform, visualize, model and communicate. Understanding develops as you cycle through these steps.\nTo do data science in R you need the R language, the RStudio IDE, the tidyverse package collection and other supporting packages. Installing and loading packages early will save time later.\nR is vectorized: arithmetic operations and functions operate on entire vectors. Use the assignment operator &lt;- to store results and choose descriptive variable names to write readable code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Session 2 – Data visualization basics",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 – Data visualization basics</span>"
    ]
  },
  {
    "objectID": "02.html#objectives",
    "href": "02.html#objectives",
    "title": "2  Session 2 – Data visualization basics",
    "section": "",
    "text": "Appreciate the importance of visualization. A simple graph can convey more information than any other device. You will learn how the grammar of graphics underlies ggplot2.\nCreate basic plots. Use ggplot2 to draw scatterplots, bar charts and histograms.\nUnderstand variable types. Recognize when to use different plot types based on whether variables are categorical or numeric.\nPrepare for layering. Today’s material sets the stage for Session 3 on layering, where you’ll add geoms, adjust positions and facet plots.\n\n\nNotes\nWhy use ggplot2? R has several systems for making graphs, but ggplot2 is one of the most elegant and versatile. It implements the grammar of graphics—a coherent system for describing and building graphs. Learning this grammar enables you to create a wide range of plots with consistent syntax.\nBuilding your first plots The data-visualization chapter starts with a simple scatterplot to illustrate aesthetic mappings and geometric objects. Aesthetic mappings tie variables to graphical properties (x and y positions, colour, shape), while geoms specify the type of plot.\n\nScatterplots show the relationship between two numeric variables. For example, plotting penguin flipper length vs. body mass can reveal whether larger penguins tend to have longer flippers. Map species or island to colour or shape to uncover additional structure.\nBar charts display the distribution of a categorical variable—for example, counting penguins by species.\nHistograms reveal the distribution of a numeric variable. Choose a bin width that balances detail with clarity.\n\nTo illustrate these concepts, load the tidyverse and palmerpenguins packages, then experiment with commands like:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n\n\n\n\nBe sure to label axes and titles with labs(), choose appropriate scales, and consider transparency (alpha) to reduce overplotting.\n\n\nKey take‑awayss\n\nggplot2 implements a coherent grammar of graphics.\nScatterplots reveal relationships between numeric variables; bar charts and histograms show distributions.\nUnderstanding your variables (categorical vs. numeric) guides your choice of plot type.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 – Data visualization basics</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "03.html#objectives",
    "href": "03.html#objectives",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "",
    "text": "Deepen your understanding of ggplot2. Explore the layered grammar of graphics—how aesthetic mappings, geometric objects and facets combine to build complex plots.\nMaster aesthetic mappings. Map variables to color, shape, size and alpha correctly. Avoid mapping categorical variables to size or alpha (it implies a false ordering) and note that mapping a categorical variable to shape uses only six shapes, so additional groups are dropped.\nLayer multiple geoms. Add multiple geoms to a plot (e.g., points and smooth lines) and distinguish between global and local aesthetic mappings. Use the group aesthetic to draw separate curves for each category.\nUse faceting and coordinate systems. Split data into panels using facet_wrap() or facet_grid() and adjust scales. Experiment with coordinate transforms such as coord_flip() and coord_polar().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "03.html#notes",
    "href": "03.html#notes",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "Notes",
    "text": "Notes\nLayered grammar of graphics – Every plot can be built from layers consisting of data, aesthetic mappings, geoms, optional statistical transformations, position adjustments and a coordinate system. Building plots layer by layer allows incremental refinement.\nAesthetic mappings – The aes() function connects variables to graphical attributes. Mapping a categorical variable to color is generally safe; mapping it to shape works but only six shapes are available; mapping to size or alpha is discouraged and yields warnings.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Mapping species to color and shape (safe if ≤ 6 categories)\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species,\n                     shape = species)) +\n  geom_point()\n\n\n\n\n\n\n\n# Mapping a categorical variable to size or alpha generates warnings\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     size = species)) +\n  geom_point()  # Warning: Using size for a discrete variable is not advised\n\n\n\n\n\n\n\n\nMappings defined in ggplot() apply globally, while mappings inside a geom override them for that layer.\n\n# Global mapping applies color to both geoms\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n# Local mapping overrides global color for the point layer only\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n\nLayering geoms – Different geoms (e.g., geom_point, geom_smooth, geom_bar) draw different types of objects. Overlaying geoms reveals multiple aspects of the data. When using geoms like geom_smooth(), ggplot2 automatically groups data by discrete variables; you can explicitly set group to control grouping.\n\n# Overlay scatterplot with separate curves per species\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(aes(group = species), method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n\nFacets – Use facet_wrap() to create a grid of subplots for one categorical variable, and facet_grid() for two variables. You can allow axes to vary across facets with the scales argument.\n\n# Facet by island\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~ island)\n\n\n\n\n\n\n\n# Facet by species and sex with free y‑axis\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  facet_grid(species ~ sex, scales = \"free_y\")\n\n\n\n\n\n\n\n\nCoordinate systems – Transform plots using different coordinate systems to improve interpretability. For example, swap axes using coord_flip() or create a polar bar chart with coord_polar().\n\n# Flip axes in a boxplot\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n# Polar coordinates for a bar chart\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  coord_polar()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Session 4 – Data transformation (I): filtering, arranging, selecting and mutating",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 – Data transformation (I): filtering, arranging, selecting and mutating</span>"
    ]
  },
  {
    "objectID": "04.html#objectives",
    "href": "04.html#objectives",
    "title": "4  Session 4 – Data transformation (I): filtering, arranging, selecting and mutating",
    "section": "",
    "text": "Understand the purpose of data transformation. You rarely get data in exactly the form needed for analysis. Transformation involves creating new variables, reordering or selecting observations, and renaming columns.\nUse dplyr verbs to manipulate rows and columns. Learn filter() to subset rows, arrange() to reorder rows, select() and rename() to choose or rename variables, and mutate() to create new columns.\nChain operations with the pipe. Use the pipe (|&gt; in base R or %&gt;% from magrittr) to express sequences of transformations in a readable way.\n\n\nNotes\nKey dplyr verbs – This session covers four of the five core verbs of dplyr:\n\nPick observations by their values: filter() subsets rows based on logical conditions.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n# Filter penguins to only Adelie species on Dream island\nadelie_dream = penguins |&gt; \n  filter(species == \"Adelie\", island == \"Dream\")\n\nadelie_dream |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Dream            39.5          16.7               178        3250\n2 Adelie  Dream            37.2          18.1               178        3900\n3 Adelie  Dream            39.5          17.8               188        3300\n4 Adelie  Dream            40.9          18.9               184        3900\n5 Adelie  Dream            36.4          17                 195        3325\n6 Adelie  Dream            39.2          21.1               196        4150\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nfilter() takes the data frame as its first argument and any number of logical expressions as additional arguments. It returns a new data frame and does not modify the original.\nReorder the rows: arrange() sorts rows by one or more variables. Use desc() for descending order and remember that missing values are sorted to the end.\n\n# Arrange penguins by body mass (descending) and flipper length (ascending)\npenguins_sorted = penguins |&gt; \n  arrange(desc(body_mass_g), flipper_length_mm)\n\npenguins_sorted |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           49.2          15.2               221        6300\n2 Gentoo  Biscoe           59.6          17                 230        6050\n3 Gentoo  Biscoe           51.1          16.3               220        6000\n4 Gentoo  Biscoe           48.8          16.2               222        6000\n5 Gentoo  Biscoe           45.2          16.4               223        5950\n6 Gentoo  Biscoe           49.8          15.9               229        5950\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nPick variables by their names: select() quickly narrows a data frame to relevant columns. It supports helper functions like starts_with(), ends_with(), and contains().\n\n# Select species, island and body_mass_g columns\npenguins_subset = penguins |&gt; \n  select(species, island, body_mass_g)\n\npenguins_subset |&gt; head()\n\n# A tibble: 6 × 3\n  species island    body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n1 Adelie  Torgersen        3750\n2 Adelie  Torgersen        3800\n3 Adelie  Torgersen        3250\n4 Adelie  Torgersen          NA\n5 Adelie  Torgersen        3450\n6 Adelie  Torgersen        3650\n\n# Select all columns except those from bill_length to bill_depth\npenguins_except = penguins |&gt; \n  select(-(bill_length_mm:bill_depth_mm))\n\npenguins_except |&gt; head()\n\n# A tibble: 6 × 6\n  species island    flipper_length_mm body_mass_g sex     year\n  &lt;fct&gt;   &lt;fct&gt;                 &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n1 Adelie  Torgersen               181        3750 male    2007\n2 Adelie  Torgersen               186        3800 female  2007\n3 Adelie  Torgersen               195        3250 female  2007\n4 Adelie  Torgersen                NA          NA &lt;NA&gt;    2007\n5 Adelie  Torgersen               193        3450 female  2007\n6 Adelie  Torgersen               190        3650 male    2007\n\n\nTo rename a column without dropping others, use rename().\n\n# Rename flipper_length_mm to flipper_mm\npenguins = penguins |&gt; \n  rename(flipper_mm = flipper_length_mm)\n\npenguins |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_mm body_mass_g sex    year\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torge…           39.1          18.7        181        3750 male   2007\n2 Adelie  Torge…           39.5          17.4        186        3800 fema…  2007\n3 Adelie  Torge…           40.3          18          195        3250 fema…  2007\n4 Adelie  Torge…           NA            NA           NA          NA &lt;NA&gt;   2007\n5 Adelie  Torge…           36.7          19.3        193        3450 fema…  2007\n6 Adelie  Torge…           39.3          20.6        190        3650 male   2007\n\n\nCreate new variables: mutate() adds new columns that are functions of existing columns. You can refer to variables created earlier in the same call.\n\n# Compute ratio of body mass to flipper length and total bill size\npenguins = penguins |&gt; \n  mutate(\n    mass_per_flipper = body_mass_g / flipper_mm,\n    bill_size = bill_length_mm + bill_depth_mm\n  )\n\npenguins |&gt; head()\n\n# A tibble: 6 × 10\n  species island bill_length_mm bill_depth_mm flipper_mm body_mass_g sex    year\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torge…           39.1          18.7        181        3750 male   2007\n2 Adelie  Torge…           39.5          17.4        186        3800 fema…  2007\n3 Adelie  Torge…           40.3          18          195        3250 fema…  2007\n4 Adelie  Torge…           NA            NA           NA          NA &lt;NA&gt;   2007\n5 Adelie  Torge…           36.7          19.3        193        3450 fema…  2007\n6 Adelie  Torge…           39.3          20.6        190        3650 male   2007\n# ℹ 2 more variables: mass_per_flipper &lt;dbl&gt;, bill_size &lt;dbl&gt;\n\n\nIf you only want to keep the new variables, use transmute().\n\nCombining operations with the pipe – Stringing multiple operations together can be awkward when saving intermediate objects. The pipe (|&gt; or %&gt;%) passes the result of one call to the next, making code easier to read.\n\n# Calculate average body mass by species in a single pipeline\nspecies_summary = penguins |&gt;\n  group_by(species) |&gt;\n  summarise(\n    count = n(),\n    mean_mass = mean(body_mass_g, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(mean_mass))\n\nspecies_summary |&gt; head()\n\n# A tibble: 3 × 3\n  species   count mean_mass\n  &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 Gentoo      124     5076.\n2 Chinstrap    68     3733.\n3 Adelie      152     3701.\n\n\nThe pipe should be read as “then”: group by species then summarise then arrange the result.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 – Data transformation (I): filtering, arranging, selecting and mutating</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "05.html#objectives",
    "href": "05.html#objectives",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "",
    "text": "Change the unit of analysis with grouping. Use group_by() to organize data into subsets (groups) so that subsequent operations operate within each group rather than on the whole data set.\nCompute summaries across groups. Apply summarize() to compute summary statistics (counts, means, medians, etc.) for each group. Understand that summarize() returns one row for each combination of grouping variables.\nUse helper functions. Learn to use helpers like n() and n_distinct() within summarize() to count observations and distinct values.\nSelect top or bottom observations per group. Use slice_max() and slice_min() to find the largest or smallest values within each group.\nEnhance pipelines. Continue to use the pipe (|&gt; or %&gt;%) to link group_by(), summarize() and other verbs into clear analytical workflows.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "05.html#notes",
    "href": "05.html#notes",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "Notes",
    "text": "Notes\nGrouping – group_by() does not change the data itself; it changes how dplyr verbs operate. After grouping, verbs like summarize(), mutate(), and filter() perform their operations separately on each group.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Group penguins by species and island\npenguins_by &lt;- penguins |&gt; group_by(species, island)\n\npenguins_by\n\n# A tibble: 344 × 8\n# Groups:   species, island [5]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSummarizing – summarize() collapses multiple rows to a single row for each group. Without any grouping, it returns a single row summarizing the entire data frame.\n\n# Summarize mean body mass and count per species-island group\npenguin_summary &lt;- penguins |&gt;\n  group_by(species, island) |&gt;\n  summarize(\n    count = n(),  # number of observations\n    mean_mass = mean(body_mass_g, na.rm = TRUE),\n    sd_mass = sd(body_mass_g, na.rm = TRUE),\n    distinct_years = n_distinct(year)\n  )\n  \n  penguin_summary\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    count mean_mass sd_mass distinct_years\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;int&gt;\n1 Adelie    Biscoe       44     3710.    488.              3\n2 Adelie    Dream        56     3688.    455.              3\n3 Adelie    Torgersen    52     3706.    445.              3\n4 Chinstrap Dream        68     3733.    384.              3\n5 Gentoo    Biscoe      124     5076.    504.              3\n\n\nIn summarize(), the summary functions must return a single value. Functions such as mean(), median(), sd() and counts like n() and n_distinct() are typical.\nUngrouping – After summarizing, grouping remains; use ungroup() if you wish to remove grouping for subsequent operations. Alternatively, the .groups argument in summarize() can control the grouping structure of the result.\nFinding extremes within groups – slice_max() and slice_min() extract rows with the highest or lowest values within each group.\n\n# Top 3 diamonds by price for each cut\ntop_diamonds &lt;- diamonds |&gt;\n  group_by(cut) |&gt;\n  slice_max(order_by = price, n = 3, with_ties = FALSE)\n\ntop_diamonds\n\n# A tibble: 15 × 10\n# Groups:   cut [5]\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.01 Fair      G     SI1      70.6    64 18574  7.43  6.64  4.69\n 2  2.02 Fair      H     VS2      64.5    57 18565  8     7.95  5.14\n 3  4.5  Fair      J     I1       65.8    58 18531 10.2  10.2   6.72\n 4  2.8  Good      G     SI2      63.8    58 18788  8.9   8.85  0   \n 5  2.07 Good      I     VS2      61.8    61 18707  8.12  8.16  5.03\n 6  2.67 Good      F     SI2      63.8    58 18686  8.69  8.64  5.54\n 7  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 8  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 9  2.03 Very Good H     SI1      63      60 18781  8     7.93  5.02\n10  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n11  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n12  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n13  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n14  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n15  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n\n\nPipelines – Continue chaining operations: group, summarize, arrange, and visualize, reading the pipe as “then”. Example:\n\n# Relationship between diamond carat and mean price by cut\ndiamond_summary &lt;- diamonds |&gt;\n  group_by(cut) |&gt;\n  summarise(\n    count = n(),\n    mean_carat = mean(carat),\n    mean_price = mean(price)\n  ) |&gt;\n  arrange(desc(mean_price))\n\ndiamond_summary |&gt; \n  ggplot(aes(x = cut, y = mean_price)) +\n  geom_col()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "06.html#objectives",
    "href": "06.html#objectives",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "",
    "text": "Read your own data into R. Up to now we have worked with datasets that come bundled with packages. In practice you will often need to import data from your own files. This session introduces the readr functions for reading plain‑text rectangular data (CSV, TSV and other delimited formats). You will learn how to specify file paths and handle column names, types and missing values.\nControl column names, missing values and types. When you run read_csv() it prints the number of rows and columns read and a summary of the column specification. You will learn how to rename variables, skip header lines, define which strings should be treated as missing, and override the type guessing heuristic that readr uses.\nRead multiple files and combine them. Real projects often involve multiple data files (for example, one file per month). You can pass a vector of file paths to read_csv() and use the id argument to record the source of each observation. We will practice finding files with list.files(), reading them into a single tibble and combining them using bind_rows().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "06.html#notes",
    "href": "06.html#notes",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "Notes",
    "text": "Notes\n\nWhy import data?\nWorking with data included in R packages is convenient when you are learning, but eventually you need to apply the tools to your own data. R for Data Science notes that this chapter focuses on reading plain‑text rectangular files and provides practical advice for handling column names, types and missing data. The goal is to help you get your data into a tidy tibble so that you can immediately start transforming and visualizing it.\n\n\nReading CSV files with readr\nThe readr package is part of the tidyverse and provides fast functions for reading delimited files. The most common function is read_csv(), which expects a path to a comma‑separated file. When you read a file, readr prints a message showing the number of rows and columns, the delimiter used and the column specification. This message includes the guessed type for each column and can be silenced with show_col_types = FALSE.\n\nlibrary(tidyverse)\n\n# Example: reading a simple CSV file from a URL\nstudents = read_csv(\"https://pos.it/r4ds-students-csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n# Suppress the column spec message\nstudents_quiet = read_csv(\"https://pos.it/r4ds-students-csv\", show_col_types = FALSE)\n\nUse the file argument to read files stored locally. It is good practice to keep data in a dedicated data/ folder within your project and refer to it using relative paths. If your file has a header row containing the names of the columns (as most CSVs do), readr uses it automatically. If not, set col_names = FALSE and optionally provide a vector of names via col_names = c(\"...\", ...).\n\n\nHandling missing values and non‑syntactic names\nA CSV file does not encode missing values explicitly, so readr treats empty fields as NA. In practice, missing values are often recorded with sentinel strings such as \"N/A\" or \".\". You can tell read_csv() which strings should be considered missing using the na argument. For example, in the students.csv file the string \"N/A\" is used to mark missing foods. By specifying na = c(\"N/A\", \"\"), readr converts those strings to NA.\n\nstudents2 = read_csv(\n  \"https://pos.it/r4ds-students-csv\",\n  na = c(\"N/A\", \"\")\n)\nstudents2\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nSometimes column names contain spaces or other characters that make them non‑syntactic in R. In that case they are surrounded by backticks in the tibble, and you must use backticks to refer to them. A simple solution is to rename the columns after import using rename() or the janitor::clean_names() helper (the latter converts names to snake_case).\n\n\nHow readr guesses column types\nBecause a CSV file does not contain type information, readr guesses the type of each column. It samples up to 1,000 values from across the file and asks: does the column contain only logical values? Only numbers? Does it match ISO8601 date format? If none of those tests succeed, readr assumes the column is a string. This heuristic works well for clean datasets but can fail when there are unexpected values (e.g., a period . to represent a missing number).\nWhen type guessing fails you can provide your own column specification via the col_types argument. This argument is a named list or a cols() specification where each name matches a column and each value is a type function (e.g., col_double(), col_integer(), col_character(), col_date(), col_datetime()). For example, if a numeric ID is misread as a number but should be a character (because you never intend to sum it), specify col_types = list(student_id = col_character()). readr provides nine column types including\n\nlogicals\ndoubles\nintegers\ncharacters\nfactors\ndates\ndate‑time\npermissive numbers\nskipped columns\n\n\n# A small CSV with a custom missing value \".\"\nsimple_csv = \"x\\n10\\n.\\n20\\n30\"\n\n# Default guess treats the period as a string\ndf_default = read_csv(simple_csv)\ndf_default\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 10   \n2 .    \n3 20   \n4 30   \n\n# Override the type to double and inspect problems\ndf_num = read_csv(simple_csv, col_types = list(x = col_double()))\nproblems(df_num)  # shows where parsing failed\n\n# A tibble: 1 × 5\n    row   col expected actual file                                              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                             \n1     3     1 a double .      C:/Users/Joshua_Patrick/AppData/Local/Temp/Rtmpcz…\n\n# Tell readr to treat \".\" as NA so guessing succeeds\ndf_fixed = read_csv(simple_csv, na = \".\")\ndf_fixed\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1    10\n2    NA\n3    20\n4    30\n\n\nIf you have many columns with the same type, you can set a default type using cols(.default = col_character()). To read only a subset of columns, use cols_only() with the columns you want.\n\n\nReading multiple files and combining them\nIn many projects you receive data split across multiple files—perhaps one file per month or per site. Instead of reading each file separately and then binding the results, you can pass a vector of file paths to read_csv(). readr will read all of them and stack the rows together. The optional id argument adds a column that records the file each row came from.\n\n# Suppose you have three monthly sales files stored in data/01-sales.csv, 02-sales.csv, 03-sales.csv\nsales_files = c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\n\n# Read and combine, adding a 'file' column to identify the source\nsales = read_csv(sales_files, id = \"file\")\n\nYou often don’t know the names of all the files ahead of time. Use list.files() to find files whose names match a pattern (e.g., \"sales\\\\.csv$\") and set full.names = TRUE so the full path is returned.\n\n# Find all CSV files ending with 'sales.csv' in the data directory\nsales_files = list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\nsales_files\nsales = read_csv(sales_files, id = \"file\")\n\nOnce you have imported multiple files you can combine them explicitly with dplyr::bind_rows() or bind_cols(), depending on whether you want to stack rows or columns. bind_rows() requires identical column names, so pay attention to names and types when reading.\n\n\nWriting data back to disk\nreadr also provides write_csv() and write_tsv() for saving tibbles to disk. Remember that when you write to CSV the column specification is lost—you must regenerate types when reading the file again. For intermediate results consider using write_rds() and read_rds(), which store R objects in a binary format preserving types.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  },
  {
    "objectID": "07.html#objectives",
    "href": "07.html#objectives",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "",
    "text": "Define tidy data. Understand the three rules that make a dataset tidy: each variable must live in its own column, each observation must occupy its own row, and each value must appear in a single cell. Appreciate why a consistent data structure makes it easier to learn and use tidyverse tools.\nLengthen data with pivot_longer(). Use pivot_longer() from the tidyr package to reshape untidy datasets by gathering column names into a new variable and their values into another variable. Learn how to select columns to pivot, specify the names of the new variables and optionally drop missing values.\nHandle multiple variables in column names. Recognize when column headers encode multiple pieces of information (e.g., method, gender and age) and use the names_to/names_sep arguments of pivot_longer() to split them into separate variables.\nWiden data with pivot_wider(). Use pivot_wider() to spread rows into columns when each observation is represented across multiple rows. Learn how to choose the names_from, values_from and id_cols arguments so that each row uniquely identifies an observation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  },
  {
    "objectID": "07.html#notes",
    "href": "07.html#notes",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "Notes",
    "text": "Notes\n\nWhat is tidy data?\nTidy data is a standard way to organize your datasets so that they work naturally with the tidyverse. In tidy data:\n\nEach variable is a column, each column is a variable. A dataset like table1 from R for Data Science has one column for each variable and is easiest to work with.\nEach observation is a row. Each row of a tidy data frame corresponds to one observation of all variables.\nEach value is a cell. Every cell contains a single value for one variable in one observation.\n\nThe pay‑off for tidying data is twofold. First, adopting a single consistent structure makes it easier to learn a suite of tools because they all assume the same underlying format. Second, placing variables in columns allows R’s vectorised functions to operate naturally.\n\n\nLengthening data with pivot_longer()\nMost real datasets aren’t tidy because they are organised for data entry or reporting rather than analysis. The pivot_longer() function lengthens data by gathering a set of columns into key–value pairs. Its most important arguments are:\n\ncols: which columns to pivot. You can specify them explicitly (bp1:bp2) or using tidyselect helpers such as starts_with().\nnames_to: the name of the new variable created from column names. For example, converting week columns (wk1, wk2, …) into a variable called week.\nvalues_to: the name of the variable that will hold the values from the pivoted columns.\nvalues_drop_na: set to TRUE to drop rows where all the pivoted columns contain NA.\n\nHere is a simple example using the built‑in billboard dataset, which records weekly Billboard chart positions. Each row is a song and each wk? column gives its rank in that week. To tidy this dataset, we gather the week columns into a new week variable and the ranks into a rank variable:\n\nlibrary(tidyverse)\nbillboard_long &lt;- billboard |&gt;\n  pivot_longer(\n    cols = starts_with(\"wk\"),\n    names_to = \"week\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt;\n  mutate(week = parse_number(week))\nbillboard_long |&gt; head()\n\n# A tibble: 6 × 5\n  artist track                   date.entered  week  rank\n  &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n\n\nThis call transforms the 317 × 79 billboard tibble into a 5307 × 5 tibble with one row per song–week combination.\nWhen column names contain multiple pieces of information, you can split them into several new variables by supplying a vector of names and a separator. For example, the who2 dataset records TB cases with column names like sp_m_014, which combine the method (sp), gender (m) and age range (014). The following call extracts those parts into separate variables:\n\nwho2_long &lt;- who2 |&gt;\n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_sep = \"_\",\n    values_to = \"count\"\n  )\nwho2_long |&gt; head()\n\n# A tibble: 6 × 6\n  country      year diagnosis gender age   count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Afghanistan  1980 sp        m      014      NA\n2 Afghanistan  1980 sp        m      1524     NA\n3 Afghanistan  1980 sp        m      2534     NA\n4 Afghanistan  1980 sp        m      3544     NA\n5 Afghanistan  1980 sp        m      4554     NA\n6 Afghanistan  1980 sp        m      5564     NA\n\n\nThe names_sep argument splits each original column name at the underscore and assigns the pieces to the new variables (diagnosis, gender, age). The values_to argument stores the counts.\n\n\nWidening data with pivot_wider()\nSometimes a single observation is spread across multiple rows, and you need to widen the data by creating new columns. The function pivot_wider() increases the number of columns and decreases the number of rows. It is particularly useful when you have long data with a variable that identifies the type of measurement and another variable with the corresponding value.\nThe key arguments are:\n\nnames_from: the column whose unique values will become new column names.\nvalues_from: the column containing the values to fill those new columns.\nid_cols: optional columns that uniquely identify each row; if you omit this, pivot_wider() will attempt to infer them but may produce duplicate rows.\n\nAs an example, the cms_patient_experience dataset from the Centers for Medicare & Medicaid Services records multiple performance measures for each healthcare organization. Each organization appears on multiple rows, one per measure. To put each organization on its own row with separate columns for each measure code, use pivot_wider():\n\ncms_wide &lt;- cms_patient_experience |&gt;\n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\ncms_wide |&gt; head()\n\n# A tibble: 6 × 8\n  org_pac_id org_nm  CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 0446157747 USC CA…          63          87          86          57          85\n2 0446162697 ASSOCI…          59          85          83          63          88\n3 0547164295 BEAVER…          49          NA          75          44          73\n4 0749333730 CAPE P…          67          84          85          65          82\n5 0840104360 ALLIAN…          66          87          87          64          87\n6 0840109864 REX HO…          73          87          84          67          91\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n\nBy specifying the identifier columns (org_pac_id and org_nm), pivot_wider() ensures that each organisation occupies a single row and creates a column for each measure code. If you omit id_cols, you may see duplicate rows because pivot_wider() cannot uniquely identify observations on its own.\n\n\nKey take‑aways\n\nTidy data has a consistent structure: variables in columns, observations in rows and values in cells. This organization simplifies analysis and leverages R’s vectorization.\npivot_longer() gathers columns into key–value pairs. Use cols to select columns to gather, names_to and values_to to name the new variables, and values_drop_na to remove structural missing values.\nSplit multiple pieces of information encoded in column names with names_to and names_sep.\npivot_wider() spreads values across new columns, reversing a pivot_longer() when observations are spread across rows. Use names_from, values_from and (optionally) id_cols to control the reshaping.\nPractice on real data. Tidy datasets are not always ready-made; most analyses require some tidying. Being proficient with pivoting functions allows you to structure your data before visualizing, transforming or modelling.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  }
]