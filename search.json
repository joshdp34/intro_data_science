[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "STA 2300 Intro to Data Science",
    "section": "",
    "text": "Introduction\nThese are the lecture notes for STA 2300 - Intro to Data Science\nPrerequisites: None\n\nCourse Description:\nPrinciples of data science, including problem workflow, variable types, visualization, modeling, programming, data management and cleaning, reproducibility, and big data.",
    "crumbs": [
      "Introduction"
    ]
  },
  {
    "objectID": "01.html",
    "href": "01.html",
    "title": "1  Session 1 – R setup",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "01.html#objectives",
    "href": "01.html#objectives",
    "title": "1  Session 1 – R setup",
    "section": "",
    "text": "Explore the data‑science lifecycle. The R for Data Science introduction models data science as a cycle: you begin by importing and tidying raw data; then understand it through an iterative loop of transforming, visualising and modelling; and finally communicate your results. Programming surrounds all of these steps and supports them. Today you will learn why each component is important and how they connect.\nRecognise the tools you need. To run the code in R for Data Science you need R, RStudio, the tidyverse package collection and a handful of other packages. We will install R (from the Comprehensive R Archive Network), download RStudio (an integrated development environment), and install the tidyverse.\nPrepare your computing environment. By the end of class you should have R and RStudio installed, know how to install and load packages, and be able to run simple R commands (arithmetic, vector creation, summary statistics). We will also introduce a dataset that you will revisit throughout the semester.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "01.html#notes",
    "href": "01.html#notes",
    "title": "1  Session 1 – R setup",
    "section": "Notes",
    "text": "Notes\n\nThe data‑science workflow\nData science is not a linear process. You first import data from files, databases or the web; tidy it so that each variable is a column and each observation a row; transform, visualise and model your data in an iterative loop to understand patterns and relationships; and then communicate your findings to others. Programming is a cross‑cutting skill that supports each of these phases. Throughout this course, you will move back and forth between these steps rather than following them in a strict order.\n\n\nTools and setup\nYou need four things to run the book’s code: R, RStudio, the tidyverse and some additional packages.\n\nR is the programming language you will use. Download the latest version from CRAN at https://cloud.r-project.org. A new major version of R is released once a year, with minor versions in between; updating regularly ensures compatibility.\n\nRStudio is an integrated development environment (IDE) for R. Download it from https://posit.co/download/rstudio-desktop/. RStudio is updated a couple of times a year. When you start RStudio, you will see a console pane for typing R code and an output pane for plots.\nThe tidyverse is a collection of packages for data manipulation, visualization and programming. To install all core tidyverse packages at once, run install.packages(\"tidyverse\") in the R console. After installation, load the tidyverse with library(tidyverse); this attaches packages such as dplyr, ggplot2, tidyr, readr, stringr, forcats, lubridate, purrr and tibble. You only need to install a package once, but you must load it in each new R session.\nOther packages. We will occasionally use packages outside the tidyverse (e.g., palmerpenguins, nycflights13, arrow, rvest, duckdb). When you encounter an error that a package is not installed, run install.packages(\"package_name\") to install the package.\n\n\n\nInstalling and testing your environment\n\nInstall R and RStudio as described above. Accept the default installation options.\n\nInstall the tidyverse. Open RStudio and run the following in the console:\n\n\ninstall.packages(\"tidyverse\")   # installs core tidyverse packages\nlibrary(tidyverse)               \n\n\nTry basic R commands. Use R as a calculator and practise creating vectors and computing summaries:\n\n\n2 + 2                  # arithmetic\n\n[1] 4\n\nx &lt;- c(1, 2, 3, 5, 7)  # create a numeric vector\nx * 2                  # vectorised multiplication\n\n[1]  2  4  6 10 14\n\nmean(x)                # compute the average\n\n[1] 3.6\n\nsum(x &gt; 4)             # count values greater than 4 (logical vector)\n\n[1] 2\n\n\nNotice that R performs operations element‑wise on vectors, and the assignment operator &lt;- stores values. Use descriptive variable names and indent your code neatly; we will discuss code style in a later class.\n\n\nTake‑aways\n\nThe data‑science process is iterative, looping through import, tidy, transform, visualize, model and communicate. Understanding develops as you cycle through these steps.\nTo do data science in R you need the R language, the RStudio IDE, the tidyverse package collection and other supporting packages. Installing and loading packages early will save time later.\nR is vectorized: arithmetic operations and functions operate on entire vectors. Use the assignment operator &lt;- to store results and choose descriptive variable names to write readable code.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Session 1 – R setup</span>"
    ]
  },
  {
    "objectID": "02.html",
    "href": "02.html",
    "title": "2  Session 2 – Data visualization basics",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 – Data visualization basics</span>"
    ]
  },
  {
    "objectID": "02.html#objectives",
    "href": "02.html#objectives",
    "title": "2  Session 2 – Data visualization basics",
    "section": "",
    "text": "Appreciate the importance of visualization. A simple graph can convey more information than any other device. You will learn how the grammar of graphics underlies ggplot2.\nCreate basic plots. Use ggplot2 to draw scatterplots, bar charts and histograms.\nUnderstand variable types. Recognize when to use different plot types based on whether variables are categorical or numeric.\nPrepare for layering. Today’s material sets the stage for Session 3 on layering, where you’ll add geoms, adjust positions and facet plots.\n\n\nNotes\nWhy use ggplot2? R has several systems for making graphs, but ggplot2 is one of the most elegant and versatile. It implements the grammar of graphics—a coherent system for describing and building graphs. Learning this grammar enables you to create a wide range of plots with consistent syntax.\nBuilding your first plots The data-visualization chapter starts with a simple scatterplot to illustrate aesthetic mappings and geometric objects. Aesthetic mappings tie variables to graphical properties (x and y positions, colour, shape), while geoms specify the type of plot.\n\nScatterplots show the relationship between two numeric variables. For example, plotting penguin flipper length vs. body mass can reveal whether larger penguins tend to have longer flippers. Map species or island to colour or shape to uncover additional structure.\nBar charts display the distribution of a categorical variable—for example, counting penguins by species.\nHistograms reveal the distribution of a numeric variable. Choose a bin width that balances detail with clarity.\n\nTo illustrate these concepts, load the tidyverse and palmerpenguins packages, then experiment with commands like:\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point()\n\n\n\n\n\n\n\nggplot(penguins, aes(x = species)) +\n  geom_bar()\n\n\n\n\n\n\n\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(binwidth = 200)\n\n\n\n\n\n\n\n\nBe sure to label axes and titles with labs(), choose appropriate scales, and consider transparency (alpha) to reduce overplotting.\n\n\nKey take‑awayss\n\nggplot2 implements a coherent grammar of graphics.\nScatterplots reveal relationships between numeric variables; bar charts and histograms show distributions.\nUnderstanding your variables (categorical vs. numeric) guides your choice of plot type.",
    "crumbs": [
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Session 2 – Data visualization basics</span>"
    ]
  },
  {
    "objectID": "03.html",
    "href": "03.html",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "03.html#objectives",
    "href": "03.html#objectives",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "",
    "text": "Deepen your understanding of ggplot2. Explore the layered grammar of graphics—how aesthetic mappings, geometric objects and facets combine to build complex plots.\nMaster aesthetic mappings. Map variables to color, shape, size and alpha correctly. Avoid mapping categorical variables to size or alpha (it implies a false ordering) and note that mapping a categorical variable to shape uses only six shapes, so additional groups are dropped.\nLayer multiple geoms. Add multiple geoms to a plot (e.g., points and smooth lines) and distinguish between global and local aesthetic mappings. Use the group aesthetic to draw separate curves for each category.\nUse faceting and coordinate systems. Split data into panels using facet_wrap() or facet_grid() and adjust scales. Experiment with coordinate transforms such as coord_flip() and coord_polar().",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "03.html#notes",
    "href": "03.html#notes",
    "title": "3  Session 3 – The layered grammar of graphics",
    "section": "Notes",
    "text": "Notes\nLayered grammar of graphics – Every plot can be built from layers consisting of data, aesthetic mappings, geoms, optional statistical transformations, position adjustments and a coordinate system. Building plots layer by layer allows incremental refinement.\nAesthetic mappings – The aes() function connects variables to graphical attributes. Mapping a categorical variable to color is generally safe; mapping it to shape works but only six shapes are available; mapping to size or alpha is discouraged and yields warnings.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Mapping species to color and shape (safe if ≤ 6 categories)\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species,\n                     shape = species)) +\n  geom_point()\n\n\n\n\n\n\n\n# Mapping a categorical variable to size or alpha generates warnings\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     size = species)) +\n  geom_point()  # Warning: Using size for a discrete variable is not advised\n\n\n\n\n\n\n\n\nMappings defined in ggplot() apply globally, while mappings inside a geom override them for that layer.\n\n# Global mapping applies color to both geoms\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, color = species)) +\n  geom_point() +\n  geom_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n# Local mapping overrides global color for the point layer only\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n\nLayering geoms – Different geoms (e.g., geom_point, geom_smooth, geom_bar) draw different types of objects. Overlaying geoms reveals multiple aspects of the data. When using geoms like geom_smooth(), ggplot2 automatically groups data by discrete variables; you can explicitly set group to control grouping.\n\n# Overlay scatterplot with separate curves per species\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(aes(color = species)) +\n  geom_smooth(aes(group = species), method = \"loess\", se = FALSE)\n\n\n\n\n\n\n\n\nFacets – Use facet_wrap() to create a grid of subplots for one categorical variable, and facet_grid() for two variables. You can allow axes to vary across facets with the scales argument.\n\n# Facet by island\nggplot(penguins, aes(x = body_mass_g, y = flipper_length_mm)) +\n  geom_point() +\n  facet_wrap(~ island)\n\n\n\n\n\n\n\n# Facet by species and sex with free y‑axis\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  facet_grid(species ~ sex, scales = \"free_y\")\n\n\n\n\n\n\n\n\nCoordinate systems – Transform plots using different coordinate systems to improve interpretability. For example, swap axes using coord_flip() or create a polar bar chart with coord_polar().\n\n# Flip axes in a boxplot\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot() +\n  coord_flip()\n\n\n\n\n\n\n\n# Polar coordinates for a bar chart\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  coord_polar()",
    "crumbs": [
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Session 3 – The layered grammar of graphics</span>"
    ]
  },
  {
    "objectID": "04.html",
    "href": "04.html",
    "title": "4  Session 4 – Data transformation (I): filtering, arranging, selecting and mutating",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 – Data transformation (I): filtering, arranging, selecting and mutating</span>"
    ]
  },
  {
    "objectID": "04.html#objectives",
    "href": "04.html#objectives",
    "title": "4  Session 4 – Data transformation (I): filtering, arranging, selecting and mutating",
    "section": "",
    "text": "Understand the purpose of data transformation. You rarely get data in exactly the form needed for analysis. Transformation involves creating new variables, reordering or selecting observations, and renaming columns.\nUse dplyr verbs to manipulate rows and columns. Learn filter() to subset rows, arrange() to reorder rows, select() and rename() to choose or rename variables, and mutate() to create new columns.\nChain operations with the pipe. Use the pipe (|&gt; in base R or %&gt;% from magrittr) to express sequences of transformations in a readable way.\n\n\nNotes\nKey dplyr verbs – This session covers four of the five core verbs of dplyr:\n\nPick observations by their values: filter() subsets rows based on logical conditions.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n# Filter penguins to only Adelie species on Dream island\nadelie_dream = penguins |&gt; \n  filter(species == \"Adelie\", island == \"Dream\")\n\nadelie_dream |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Dream            39.5          16.7               178        3250\n2 Adelie  Dream            37.2          18.1               178        3900\n3 Adelie  Dream            39.5          17.8               188        3300\n4 Adelie  Dream            40.9          18.9               184        3900\n5 Adelie  Dream            36.4          17                 195        3325\n6 Adelie  Dream            39.2          21.1               196        4150\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nfilter() takes the data frame as its first argument and any number of logical expressions as additional arguments. It returns a new data frame and does not modify the original.\nReorder the rows: arrange() sorts rows by one or more variables. Use desc() for descending order and remember that missing values are sorted to the end.\n\n# Arrange penguins by body mass (descending) and flipper length (ascending)\npenguins_sorted = penguins |&gt; \n  arrange(desc(body_mass_g), flipper_length_mm)\n\npenguins_sorted |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Gentoo  Biscoe           49.2          15.2               221        6300\n2 Gentoo  Biscoe           59.6          17                 230        6050\n3 Gentoo  Biscoe           51.1          16.3               220        6000\n4 Gentoo  Biscoe           48.8          16.2               222        6000\n5 Gentoo  Biscoe           45.2          16.4               223        5950\n6 Gentoo  Biscoe           49.8          15.9               229        5950\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nPick variables by their names: select() quickly narrows a data frame to relevant columns. It supports helper functions like starts_with(), ends_with(), and contains().\n\n# Select species, island and body_mass_g columns\npenguins_subset = penguins |&gt; \n  select(species, island, body_mass_g)\n\npenguins_subset |&gt; head()\n\n# A tibble: 6 × 3\n  species island    body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n1 Adelie  Torgersen        3750\n2 Adelie  Torgersen        3800\n3 Adelie  Torgersen        3250\n4 Adelie  Torgersen          NA\n5 Adelie  Torgersen        3450\n6 Adelie  Torgersen        3650\n\n# Select all columns except those from bill_length to bill_depth\npenguins_except = penguins |&gt; \n  select(-(bill_length_mm:bill_depth_mm))\n\npenguins_except |&gt; head()\n\n# A tibble: 6 × 6\n  species island    flipper_length_mm body_mass_g sex     year\n  &lt;fct&gt;   &lt;fct&gt;                 &lt;int&gt;       &lt;int&gt; &lt;fct&gt;  &lt;int&gt;\n1 Adelie  Torgersen               181        3750 male    2007\n2 Adelie  Torgersen               186        3800 female  2007\n3 Adelie  Torgersen               195        3250 female  2007\n4 Adelie  Torgersen                NA          NA &lt;NA&gt;    2007\n5 Adelie  Torgersen               193        3450 female  2007\n6 Adelie  Torgersen               190        3650 male    2007\n\n\nTo rename a column without dropping others, use rename().\n\n# Rename flipper_length_mm to flipper_mm\npenguins = penguins |&gt; \n  rename(flipper_mm = flipper_length_mm)\n\npenguins |&gt; head()\n\n# A tibble: 6 × 8\n  species island bill_length_mm bill_depth_mm flipper_mm body_mass_g sex    year\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torge…           39.1          18.7        181        3750 male   2007\n2 Adelie  Torge…           39.5          17.4        186        3800 fema…  2007\n3 Adelie  Torge…           40.3          18          195        3250 fema…  2007\n4 Adelie  Torge…           NA            NA           NA          NA &lt;NA&gt;   2007\n5 Adelie  Torge…           36.7          19.3        193        3450 fema…  2007\n6 Adelie  Torge…           39.3          20.6        190        3650 male   2007\n\n\nCreate new variables: mutate() adds new columns that are functions of existing columns. You can refer to variables created earlier in the same call.\n\n# Compute ratio of body mass to flipper length and total bill size\npenguins = penguins |&gt; \n  mutate(\n    mass_per_flipper = body_mass_g / flipper_mm,\n    bill_size = bill_length_mm + bill_depth_mm\n  )\n\npenguins |&gt; head()\n\n# A tibble: 6 × 10\n  species island bill_length_mm bill_depth_mm flipper_mm body_mass_g sex    year\n  &lt;fct&gt;   &lt;fct&gt;           &lt;dbl&gt;         &lt;dbl&gt;      &lt;int&gt;       &lt;int&gt; &lt;fct&gt; &lt;int&gt;\n1 Adelie  Torge…           39.1          18.7        181        3750 male   2007\n2 Adelie  Torge…           39.5          17.4        186        3800 fema…  2007\n3 Adelie  Torge…           40.3          18          195        3250 fema…  2007\n4 Adelie  Torge…           NA            NA           NA          NA &lt;NA&gt;   2007\n5 Adelie  Torge…           36.7          19.3        193        3450 fema…  2007\n6 Adelie  Torge…           39.3          20.6        190        3650 male   2007\n# ℹ 2 more variables: mass_per_flipper &lt;dbl&gt;, bill_size &lt;dbl&gt;\n\n\nIf you only want to keep the new variables, use transmute().\n\nCombining operations with the pipe – Stringing multiple operations together can be awkward when saving intermediate objects. The pipe (|&gt; or %&gt;%) passes the result of one call to the next, making code easier to read.\n\n# Calculate average body mass by species in a single pipeline\nspecies_summary = penguins |&gt;\n  group_by(species) |&gt;\n  summarise(\n    count = n(),\n    mean_mass = mean(body_mass_g, na.rm = TRUE)\n  ) |&gt;\n  arrange(desc(mean_mass))\n\nspecies_summary |&gt; head()\n\n# A tibble: 3 × 3\n  species   count mean_mass\n  &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;\n1 Gentoo      124     5076.\n2 Chinstrap    68     3733.\n3 Adelie      152     3701.\n\n\nThe pipe should be read as “then”: group by species then summarise then arrange the result.",
    "crumbs": [
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Session 4 – Data transformation (I): filtering, arranging, selecting and mutating</span>"
    ]
  },
  {
    "objectID": "05.html",
    "href": "05.html",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "05.html#objectives",
    "href": "05.html#objectives",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "",
    "text": "Change the unit of analysis with grouping. Use group_by() to organize data into subsets (groups) so that subsequent operations operate within each group rather than on the whole data set.\nCompute summaries across groups. Apply summarize() to compute summary statistics (counts, means, medians, etc.) for each group. Understand that summarize() returns one row for each combination of grouping variables.\nUse helper functions. Learn to use helpers like n() and n_distinct() within summarize() to count observations and distinct values.\nSelect top or bottom observations per group. Use slice_max() and slice_min() to find the largest or smallest values within each group.\nEnhance pipelines. Continue to use the pipe (|&gt; or %&gt;%) to link group_by(), summarize() and other verbs into clear analytical workflows.",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "05.html#notes",
    "href": "05.html#notes",
    "title": "5  Session 5 – Data transformation (II): grouping and summarizing",
    "section": "Notes",
    "text": "Notes\nGrouping – group_by() does not change the data itself; it changes how dplyr verbs operate. After grouping, verbs like summarize(), mutate(), and filter() perform their operations separately on each group.\n\nlibrary(tidyverse)\nlibrary(palmerpenguins)\n\n# Group penguins by species and island\npenguins_by &lt;- penguins |&gt; group_by(species, island)\n\npenguins_by\n\n# A tibble: 344 × 8\n# Groups:   species, island [5]\n   species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n   &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n 1 Adelie  Torgersen           39.1          18.7               181        3750\n 2 Adelie  Torgersen           39.5          17.4               186        3800\n 3 Adelie  Torgersen           40.3          18                 195        3250\n 4 Adelie  Torgersen           NA            NA                  NA          NA\n 5 Adelie  Torgersen           36.7          19.3               193        3450\n 6 Adelie  Torgersen           39.3          20.6               190        3650\n 7 Adelie  Torgersen           38.9          17.8               181        3625\n 8 Adelie  Torgersen           39.2          19.6               195        4675\n 9 Adelie  Torgersen           34.1          18.1               193        3475\n10 Adelie  Torgersen           42            20.2               190        4250\n# ℹ 334 more rows\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nSummarizing – summarize() collapses multiple rows to a single row for each group. Without any grouping, it returns a single row summarizing the entire data frame.\n\n# Summarize mean body mass and count per species-island group\npenguin_summary &lt;- penguins |&gt;\n  group_by(species, island) |&gt;\n  summarize(\n    count = n(),  # number of observations\n    mean_mass = mean(body_mass_g, na.rm = TRUE),\n    sd_mass = sd(body_mass_g, na.rm = TRUE),\n    distinct_years = n_distinct(year)\n  )\n  \n  penguin_summary\n\n# A tibble: 5 × 6\n# Groups:   species [3]\n  species   island    count mean_mass sd_mass distinct_years\n  &lt;fct&gt;     &lt;fct&gt;     &lt;int&gt;     &lt;dbl&gt;   &lt;dbl&gt;          &lt;int&gt;\n1 Adelie    Biscoe       44     3710.    488.              3\n2 Adelie    Dream        56     3688.    455.              3\n3 Adelie    Torgersen    52     3706.    445.              3\n4 Chinstrap Dream        68     3733.    384.              3\n5 Gentoo    Biscoe      124     5076.    504.              3\n\n\nIn summarize(), the summary functions must return a single value. Functions such as mean(), median(), sd() and counts like n() and n_distinct() are typical.\nUngrouping – After summarizing, grouping remains; use ungroup() if you wish to remove grouping for subsequent operations. Alternatively, the .groups argument in summarize() can control the grouping structure of the result.\nFinding extremes within groups – slice_max() and slice_min() extract rows with the highest or lowest values within each group.\n\n# Top 3 diamonds by price for each cut\ntop_diamonds &lt;- diamonds |&gt;\n  group_by(cut) |&gt;\n  slice_max(order_by = price, n = 3, with_ties = FALSE)\n\ntop_diamonds\n\n# A tibble: 15 × 10\n# Groups:   cut [5]\n   carat cut       color clarity depth table price     x     y     z\n   &lt;dbl&gt; &lt;ord&gt;     &lt;ord&gt; &lt;ord&gt;   &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n 1  2.01 Fair      G     SI1      70.6    64 18574  7.43  6.64  4.69\n 2  2.02 Fair      H     VS2      64.5    57 18565  8     7.95  5.14\n 3  4.5  Fair      J     I1       65.8    58 18531 10.2  10.2   6.72\n 4  2.8  Good      G     SI2      63.8    58 18788  8.9   8.85  0   \n 5  2.07 Good      I     VS2      61.8    61 18707  8.12  8.16  5.03\n 6  2.67 Good      F     SI2      63.8    58 18686  8.69  8.64  5.54\n 7  2    Very Good G     SI1      63.5    56 18818  7.9   7.97  5.04\n 8  2    Very Good H     SI1      62.8    57 18803  7.95  8     5.01\n 9  2.03 Very Good H     SI1      63      60 18781  8     7.93  5.02\n10  2.29 Premium   I     VS2      60.8    60 18823  8.5   8.47  5.16\n11  2.29 Premium   I     SI1      61.8    59 18797  8.52  8.45  5.24\n12  2.04 Premium   H     SI1      58.1    60 18795  8.37  8.28  4.84\n13  1.51 Ideal     G     IF       61.7    55 18806  7.37  7.41  4.56\n14  2.07 Ideal     G     SI2      62.5    55 18804  8.2   8.13  5.11\n15  2.15 Ideal     G     SI2      62.6    54 18791  8.29  8.35  5.21\n\n\nPipelines – Continue chaining operations: group, summarize, arrange, and visualize, reading the pipe as “then”. Example:\n\n# Relationship between diamond carat and mean price by cut\ndiamond_summary &lt;- diamonds |&gt;\n  group_by(cut) |&gt;\n  summarise(\n    count = n(),\n    mean_carat = mean(carat),\n    mean_price = mean(price)\n  ) |&gt;\n  arrange(desc(mean_price))\n\ndiamond_summary |&gt; \n  ggplot(aes(x = cut, y = mean_price)) +\n  geom_col()",
    "crumbs": [
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Session 5 – Data transformation (II): grouping and summarizing</span>"
    ]
  },
  {
    "objectID": "06.html",
    "href": "06.html",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "06.html#objectives",
    "href": "06.html#objectives",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "",
    "text": "Read your own data into R. Up to now we have worked with datasets that come bundled with packages. In practice you will often need to import data from your own files. This session introduces the readr functions for reading plain‑text rectangular data (CSV, TSV and other delimited formats). You will learn how to specify file paths and handle column names, types and missing values.\nControl column names, missing values and types. When you run read_csv() it prints the number of rows and columns read and a summary of the column specification. You will learn how to rename variables, skip header lines, define which strings should be treated as missing, and override the type guessing heuristic that readr uses.\nRead multiple files and combine them. Real projects often involve multiple data files (for example, one file per month). You can pass a vector of file paths to read_csv() and use the id argument to record the source of each observation. We will practice finding files with list.files(), reading them into a single tibble and combining them using bind_rows().",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "06.html#notes",
    "href": "06.html#notes",
    "title": "6  Session 6 – Data import: reading CSV & flat files",
    "section": "Notes",
    "text": "Notes\n\nWhy import data?\nWorking with data included in R packages is convenient when you are learning, but eventually you need to apply the tools to your own data. R for Data Science notes that this chapter focuses on reading plain‑text rectangular files and provides practical advice for handling column names, types and missing data. The goal is to help you get your data into a tidy tibble so that you can immediately start transforming and visualizing it.\n\n\nReading CSV files with readr\nThe readr package is part of the tidyverse and provides fast functions for reading delimited files. The most common function is read_csv(), which expects a path to a comma‑separated file. When you read a file, readr prints a message showing the number of rows and columns, the delimiter used and the column specification. This message includes the guessed type for each column and can be silenced with show_col_types = FALSE.\n\nlibrary(tidyverse)\n\n# Example: reading a simple CSV file from a URL\nstudents = read_csv(\"https://pos.it/r4ds-students-csv\")\nstudents\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    N/A                Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n# Suppress the column spec message\nstudents_quiet = read_csv(\"https://pos.it/r4ds-students-csv\", show_col_types = FALSE)\n\nUse the file argument to read files stored locally. It is good practice to keep data in a dedicated data/ folder within your project and refer to it using relative paths. If your file has a header row containing the names of the columns (as most CSVs do), readr uses it automatically. If not, set col_names = FALSE and optionally provide a vector of names via col_names = c(\"...\", ...).\n\n\nHandling missing values and non‑syntactic names\nA CSV file does not encode missing values explicitly, so readr treats empty fields as NA. In practice, missing values are often recorded with sentinel strings such as \"N/A\" or \".\". You can tell read_csv() which strings should be considered missing using the na argument. For example, in the students.csv file the string \"N/A\" is used to mark missing foods. By specifying na = c(\"N/A\", \"\"), readr converts those strings to NA.\n\nstudents2 = read_csv(\n  \"https://pos.it/r4ds-students-csv\",\n  na = c(\"N/A\", \"\")\n)\nstudents2\n\n# A tibble: 6 × 5\n  `Student ID` `Full Name`      favourite.food     mealPlan            AGE  \n         &lt;dbl&gt; &lt;chr&gt;            &lt;chr&gt;              &lt;chr&gt;               &lt;chr&gt;\n1            1 Sunil Huffmann   Strawberry yoghurt Lunch only          4    \n2            2 Barclay Lynn     French fries       Lunch only          5    \n3            3 Jayendra Lyne    &lt;NA&gt;               Breakfast and lunch 7    \n4            4 Leon Rossini     Anchovies          Lunch only          &lt;NA&gt; \n5            5 Chidiegwu Dunkel Pizza              Breakfast and lunch five \n6            6 Güvenç Attila    Ice cream          Lunch only          6    \n\n\nSometimes column names contain spaces or other characters that make them non‑syntactic in R. In that case they are surrounded by backticks in the tibble, and you must use backticks to refer to them. A simple solution is to rename the columns after import using rename() or the janitor::clean_names() helper (the latter converts names to snake_case).\n\n\nHow readr guesses column types\nBecause a CSV file does not contain type information, readr guesses the type of each column. It samples up to 1,000 values from across the file and asks: does the column contain only logical values? Only numbers? Does it match ISO8601 date format? If none of those tests succeed, readr assumes the column is a string. This heuristic works well for clean datasets but can fail when there are unexpected values (e.g., a period . to represent a missing number).\nWhen type guessing fails you can provide your own column specification via the col_types argument. This argument is a named list or a cols() specification where each name matches a column and each value is a type function (e.g., col_double(), col_integer(), col_character(), col_date(), col_datetime()). For example, if a numeric ID is misread as a number but should be a character (because you never intend to sum it), specify col_types = list(student_id = col_character()). readr provides nine column types including\n\nlogicals\ndoubles\nintegers\ncharacters\nfactors\ndates\ndate‑time\npermissive numbers\nskipped columns\n\n\n# A small CSV with a custom missing value \".\"\nsimple_csv = \"x\\n10\\n.\\n20\\n30\"\n\n# Default guess treats the period as a string\ndf_default = read_csv(simple_csv)\ndf_default\n\n# A tibble: 4 × 1\n  x    \n  &lt;chr&gt;\n1 10   \n2 .    \n3 20   \n4 30   \n\n# Override the type to double and inspect problems\ndf_num = read_csv(simple_csv, col_types = list(x = col_double()))\nproblems(df_num)  # shows where parsing failed\n\n# A tibble: 1 × 5\n    row   col expected actual file                                              \n  &lt;int&gt; &lt;int&gt; &lt;chr&gt;    &lt;chr&gt;  &lt;chr&gt;                                             \n1     3     1 a double .      C:/Users/Joshua_Patrick/AppData/Local/Temp/Rtmp6b…\n\n# Tell readr to treat \".\" as NA so guessing succeeds\ndf_fixed = read_csv(simple_csv, na = \".\")\ndf_fixed\n\n# A tibble: 4 × 1\n      x\n  &lt;dbl&gt;\n1    10\n2    NA\n3    20\n4    30\n\n\nIf you have many columns with the same type, you can set a default type using cols(.default = col_character()). To read only a subset of columns, use cols_only() with the columns you want.\n\n\nReading multiple files and combining them\nIn many projects you receive data split across multiple files—perhaps one file per month or per site. Instead of reading each file separately and then binding the results, you can pass a vector of file paths to read_csv(). readr will read all of them and stack the rows together. The optional id argument adds a column that records the file each row came from.\n\n# Suppose you have three monthly sales files stored in data/01-sales.csv, 02-sales.csv, 03-sales.csv\nsales_files = c(\"data/01-sales.csv\", \"data/02-sales.csv\", \"data/03-sales.csv\")\n\n# Read and combine, adding a 'file' column to identify the source\nsales = read_csv(sales_files, id = \"file\")\n\nYou often don’t know the names of all the files ahead of time. Use list.files() to find files whose names match a pattern (e.g., \"sales\\\\.csv$\") and set full.names = TRUE so the full path is returned.\n\n# Find all CSV files ending with 'sales.csv' in the data directory\nsales_files = list.files(\"data\", pattern = \"sales\\\\.csv$\", full.names = TRUE)\nsales_files\nsales = read_csv(sales_files, id = \"file\")\n\nOnce you have imported multiple files you can combine them explicitly with dplyr::bind_rows() or bind_cols(), depending on whether you want to stack rows or columns. bind_rows() requires identical column names, so pay attention to names and types when reading.\n\n\nWriting data back to disk\nreadr also provides write_csv() and write_tsv() for saving tibbles to disk. Remember that when you write to CSV the column specification is lost—you must regenerate types when reading the file again. For intermediate results consider using write_rds() and read_rds(), which store R objects in a binary format preserving types.",
    "crumbs": [
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Session 6 – Data import: reading CSV & flat files</span>"
    ]
  },
  {
    "objectID": "07.html",
    "href": "07.html",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  },
  {
    "objectID": "07.html#objectives",
    "href": "07.html#objectives",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "",
    "text": "Define tidy data. Understand the three rules that make a dataset tidy: each variable must live in its own column, each observation must occupy its own row, and each value must appear in a single cell. Appreciate why a consistent data structure makes it easier to learn and use tidyverse tools.\nLengthen data with pivot_longer(). Use pivot_longer() from the tidyr package to reshape untidy datasets by gathering column names into a new variable and their values into another variable. Learn how to select columns to pivot, specify the names of the new variables and optionally drop missing values.\nHandle multiple variables in column names. Recognize when column headers encode multiple pieces of information (e.g., method, gender and age) and use the names_to/names_sep arguments of pivot_longer() to split them into separate variables.\nWiden data with pivot_wider(). Use pivot_wider() to spread rows into columns when each observation is represented across multiple rows. Learn how to choose the names_from, values_from and id_cols arguments so that each row uniquely identifies an observation.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  },
  {
    "objectID": "07.html#notes",
    "href": "07.html#notes",
    "title": "7  Session 7 – Tidy data & pivoting",
    "section": "Notes",
    "text": "Notes\n\nWhat is tidy data?\nTidy data is a standard way to organize your datasets so that they work naturally with the tidyverse. In tidy data:\n\nEach variable is a column, each column is a variable. A dataset like table1 from R for Data Science has one column for each variable and is easiest to work with.\nEach observation is a row. Each row of a tidy data frame corresponds to one observation of all variables.\nEach value is a cell. Every cell contains a single value for one variable in one observation.\n\nThe pay‑off for tidying data is twofold. First, adopting a single consistent structure makes it easier to learn a suite of tools because they all assume the same underlying format. Second, placing variables in columns allows R’s vectorised functions to operate naturally.\n\n\nLengthening data with pivot_longer()\nMost real datasets aren’t tidy because they are organised for data entry or reporting rather than analysis. The pivot_longer() function lengthens data by gathering a set of columns into key–value pairs. Its most important arguments are:\n\ncols: which columns to pivot. You can specify them explicitly (bp1:bp2) or using tidyselect helpers such as starts_with().\nnames_to: the name of the new variable created from column names. For example, converting week columns (wk1, wk2, …) into a variable called week.\nvalues_to: the name of the variable that will hold the values from the pivoted columns.\nvalues_drop_na: set to TRUE to drop rows where all the pivoted columns contain NA.\n\nHere is a simple example using the built‑in billboard dataset, which records weekly Billboard chart positions. Each row is a song and each wk? column gives its rank in that week. To tidy this dataset, we gather the week columns into a new week variable and the ranks into a rank variable:\n\nlibrary(tidyverse)\nbillboard_long &lt;- billboard |&gt;\n  pivot_longer(\n    cols = starts_with(\"wk\"),\n    names_to = \"week\",\n    values_to = \"rank\",\n    values_drop_na = TRUE\n  ) |&gt;\n  mutate(week = parse_number(week))\nbillboard_long |&gt; head()\n\n# A tibble: 6 × 5\n  artist track                   date.entered  week  rank\n  &lt;chr&gt;  &lt;chr&gt;                   &lt;date&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 2 Pac  Baby Don't Cry (Keep... 2000-02-26       1    87\n2 2 Pac  Baby Don't Cry (Keep... 2000-02-26       2    82\n3 2 Pac  Baby Don't Cry (Keep... 2000-02-26       3    72\n4 2 Pac  Baby Don't Cry (Keep... 2000-02-26       4    77\n5 2 Pac  Baby Don't Cry (Keep... 2000-02-26       5    87\n6 2 Pac  Baby Don't Cry (Keep... 2000-02-26       6    94\n\n\nThis call transforms the 317 × 79 billboard tibble into a 5307 × 5 tibble with one row per song–week combination.\nWhen column names contain multiple pieces of information, you can split them into several new variables by supplying a vector of names and a separator. For example, the who2 dataset records TB cases with column names like sp_m_014, which combine the method (sp), gender (m) and age range (014). The following call extracts those parts into separate variables:\n\nwho2_long &lt;- who2 |&gt;\n  pivot_longer(\n    cols = !(country:year),\n    names_to = c(\"diagnosis\", \"gender\", \"age\"),\n    names_sep = \"_\",\n    values_to = \"count\"\n  )\nwho2_long |&gt; head()\n\n# A tibble: 6 × 6\n  country      year diagnosis gender age   count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Afghanistan  1980 sp        m      014      NA\n2 Afghanistan  1980 sp        m      1524     NA\n3 Afghanistan  1980 sp        m      2534     NA\n4 Afghanistan  1980 sp        m      3544     NA\n5 Afghanistan  1980 sp        m      4554     NA\n6 Afghanistan  1980 sp        m      5564     NA\n\n\nThe names_sep argument splits each original column name at the underscore and assigns the pieces to the new variables (diagnosis, gender, age). The values_to argument stores the counts.\n\n\nWidening data with pivot_wider()\nSometimes a single observation is spread across multiple rows, and you need to widen the data by creating new columns. The function pivot_wider() increases the number of columns and decreases the number of rows. It is particularly useful when you have long data with a variable that identifies the type of measurement and another variable with the corresponding value.\nThe key arguments are:\n\nnames_from: the column whose unique values will become new column names.\nvalues_from: the column containing the values to fill those new columns.\nid_cols: optional columns that uniquely identify each row; if you omit this, pivot_wider() will attempt to infer them but may produce duplicate rows.\n\nAs an example, the cms_patient_experience dataset from the Centers for Medicare & Medicaid Services records multiple performance measures for each healthcare organization. Each organization appears on multiple rows, one per measure. To put each organization on its own row with separate columns for each measure code, use pivot_wider():\n\ncms_wide &lt;- cms_patient_experience |&gt;\n  pivot_wider(\n    id_cols = starts_with(\"org\"),\n    names_from = measure_cd,\n    values_from = prf_rate\n  )\ncms_wide |&gt; head()\n\n# A tibble: 6 × 8\n  org_pac_id org_nm  CAHPS_GRP_1 CAHPS_GRP_2 CAHPS_GRP_3 CAHPS_GRP_5 CAHPS_GRP_8\n  &lt;chr&gt;      &lt;chr&gt;         &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n1 0446157747 USC CA…          63          87          86          57          85\n2 0446162697 ASSOCI…          59          85          83          63          88\n3 0547164295 BEAVER…          49          NA          75          44          73\n4 0749333730 CAPE P…          67          84          85          65          82\n5 0840104360 ALLIAN…          66          87          87          64          87\n6 0840109864 REX HO…          73          87          84          67          91\n# ℹ 1 more variable: CAHPS_GRP_12 &lt;dbl&gt;\n\n\nBy specifying the identifier columns (org_pac_id and org_nm), pivot_wider() ensures that each organisation occupies a single row and creates a column for each measure code. If you omit id_cols, you may see duplicate rows because pivot_wider() cannot uniquely identify observations on its own.\n\n\nKey take‑aways\n\nTidy data has a consistent structure: variables in columns, observations in rows and values in cells. This organization simplifies analysis and leverages R’s vectorization.\npivot_longer() gathers columns into key–value pairs. Use cols to select columns to gather, names_to and values_to to name the new variables, and values_drop_na to remove structural missing values.\nSplit multiple pieces of information encoded in column names with names_to and names_sep.\npivot_wider() spreads values across new columns, reversing a pivot_longer() when observations are spread across rows. Use names_from, values_from and (optionally) id_cols to control the reshaping.\nPractice on real data. Tidy datasets are not always ready-made; most analyses require some tidying. Being proficient with pivoting functions allows you to structure your data before visualizing, transforming or modelling.",
    "crumbs": [
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Session 7 – Tidy data & pivoting</span>"
    ]
  },
  {
    "objectID": "08.html",
    "href": "08.html",
    "title": "8  Session 8 – Variable types I: logical & numeric vectors",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 – Variable types I: logical & numeric vectors</span>"
    ]
  },
  {
    "objectID": "08.html#objectives",
    "href": "08.html#objectives",
    "title": "8  Session 8 – Variable types I: logical & numeric vectors",
    "section": "",
    "text": "Distinguish logical and numeric vectors. Logical vectors contain only TRUE, FALSE or NA, whereas numeric vectors contain integers or doubles. Learn to convert strings to numbers with parse_double() and parse_number().\nCreate and combine logical vectors. Use comparison operators (&lt;, &lt;=, &gt;, &gt;=, !=, ==) to generate logical vectors and combine them with &, |, ! and xor(). Avoid short‑circuiting operators (&&, ||) inside data‑masking verbs. Understand how missing values propagate through comparisons and Boolean operations.\nSummarize logical vectors. Collapse logical vectors with any() and all() or coerce them to numeric and use sum()/mean() to count or compute the proportion of TRUEs. Detect missing values with is.na().\nPerform numeric operations and transformations. Recognize that R recycles shorter vectors in arithmetic. Use pmin()/pmax() for element‑wise minima and maxima, %/% and %% for modular arithmetic, logarithms for rescaling, round(), floor() and ceiling() for rounding, and cut() to bin numeric data.\nCompute numeric summaries. Compare the mean and median—the mean is sensitive to extreme values while the median is robust. Use quantiles to summarize tails and spread measures such as the standard deviation and interquartile range.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 – Variable types I: logical & numeric vectors</span>"
    ]
  },
  {
    "objectID": "08.html#notes",
    "href": "08.html#notes",
    "title": "8  Session 8 – Variable types I: logical & numeric vectors",
    "section": "Notes",
    "text": "Notes\n\nLogical vectors and comparisons\nLogical vectors are created by comparing values. They can take on three values: TRUE, FALSE or NA. Comparison operators return a logical vector with the same length as the input. For example:\n\nlibrary(tidyverse)\nlibrary(nycflights13)\n\nflights |&gt;\n  mutate(late_dep = dep_delay &gt; 30,      # TRUE if departure delay &gt; 30 min\n         early_arr = arr_delay &lt; 0) |&gt;   # TRUE if arrival was early\n  select(year:day, dep_delay, arr_delay, late_dep, early_arr) |&gt;\n  head(5)\n\n# A tibble: 5 × 7\n   year month   day dep_delay arr_delay late_dep early_arr\n  &lt;int&gt; &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;     &lt;dbl&gt; &lt;lgl&gt;    &lt;lgl&gt;    \n1  2013     1     1         2        11 FALSE    FALSE    \n2  2013     1     1         4        20 FALSE    FALSE    \n3  2013     1     1         2        33 FALSE    FALSE    \n4  2013     1     1        -1       -18 FALSE    TRUE     \n5  2013     1     1        -6       -25 FALSE    TRUE     \n\n\nBe cautious when comparing floating‑point numbers. Tiny rounding errors mean that equality tests may fail. To check approximate equality use dplyr::near().\n\n\nBoolean algebra and missing values\nCombine logical vectors using Boolean algebra. In R, & means and, | means or, ! means not, and xor() is exclusive‑or. The short‑circuiting operators && and || return a single TRUE/FALSE and should not be used inside functions like filter().\nMissing values are contagious: any comparison involving NA yields NA. In Boolean operations, NA | TRUE evaluates to TRUE but NA | FALSE remains NA. Use is.na() to test for missing values:\n\nflights |&gt; \n  summarise(\n    n_missing_dep = sum(is.na(dep_time)),\n    n_missing_arr = sum(is.na(arr_time))\n  )\n\n# A tibble: 1 × 2\n  n_missing_dep n_missing_arr\n          &lt;int&gt;         &lt;int&gt;\n1          8255          8713\n\n\nWhen combining conditions, remember the order of operations. To filter flights departing in November or December, write month == 11 | month == 12 or use %in%; writing month == 11 | 12 mistakenly recycles the vector c(11, 12).\n\n\nSummarising logical vectors\nLogical summaries collapse a logical vector to a single value. any(x) returns TRUE if any element of x is TRUE, while all(x) returns TRUE only if every element is TRUE. Both functions accept na.rm = TRUE to ignore missing values. For example, to check if all flights on a given day departed within an hour or if any flights arrived more than five hours late:\n\nflights |&gt;\n  group_by(year, month, day) |&gt;\n  summarise(\n    all_dep_within_hour = all(dep_delay &lt;= 60, na.rm = TRUE),\n    any_long_arr_delay  = any(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# A tibble: 365 × 5\n    year month   day all_dep_within_hour any_long_arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;lgl&gt;               &lt;lgl&gt;             \n 1  2013     1     1 FALSE               TRUE              \n 2  2013     1     2 FALSE               TRUE              \n 3  2013     1     3 FALSE               FALSE             \n 4  2013     1     4 FALSE               FALSE             \n 5  2013     1     5 FALSE               TRUE              \n 6  2013     1     6 FALSE               FALSE             \n 7  2013     1     7 FALSE               TRUE              \n 8  2013     1     8 FALSE               FALSE             \n 9  2013     1     9 FALSE               TRUE              \n10  2013     1    10 FALSE               TRUE              \n# ℹ 355 more rows\n\n\nSince logical vectors coerce to numeric (TRUE = 1, FALSE = 0), sum() counts the number of TRUE values and mean() computes their proportion. For instance, the proportion of flights departing within an hour and the number with very long arrival delays can be calculated by:\n\nflights |&gt;\n  group_by(year, month, day) |&gt;\n  summarise(\n    prop_on_time_dep    = mean(dep_delay &lt;= 60, na.rm = TRUE),\n    count_long_arr_delay = sum(arr_delay &gt;= 300, na.rm = TRUE),\n    .groups = \"drop\"\n  )\n\n# A tibble: 365 × 5\n    year month   day prop_on_time_dep count_long_arr_delay\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;            &lt;dbl&gt;                &lt;int&gt;\n 1  2013     1     1            0.939                    3\n 2  2013     1     2            0.914                    3\n 3  2013     1     3            0.941                    0\n 4  2013     1     4            0.953                    0\n 5  2013     1     5            0.964                    1\n 6  2013     1     6            0.959                    0\n 7  2013     1     7            0.956                    1\n 8  2013     1     8            0.975                    0\n 9  2013     1     9            0.986                    1\n10  2013     1    10            0.977                    2\n# ℹ 355 more rows\n\n\nLogical vectors also enable inline subsetting. Rather than filtering the entire data frame, you can subset a single vector with a logical condition (e.g., arr_delay[arr_delay &gt; 0]) to compute summaries only on values meeting a criterion.\n\n\nNumeric vectors: making numbers and parsing\nNumeric vectors may be integers or doubles. When numbers are stored as text (e.g., \"$1,234\" or \"59%\"), use readr::parse_double() to convert simple numeric strings and readr::parse_number() to strip extraneous characters like currency symbols or percent signs.\n\n\nVectorized arithmetic and recycling\nR performs arithmetic element‑wise and recycles the shorter vector to match the length of the longer one. Recycling single numbers (e.g., x / 5) is convenient, but recycling longer vectors can produce warnings or silent errors. Avoid using == with vectors of unequal length; instead use %in% when matching multiple values.\nElement‑wise minima and maxima are computed with pmin() and pmax(). Modular arithmetic operators %/% and %% perform integer division and find remainders. For example, unpacking a four‑digit departure time into hours and minutes:\n\nflights |&gt;\n  mutate(\n    sched_hour   = sched_dep_time %/% 100,\n    sched_minute = sched_dep_time %% 100\n  ) |&gt;\n  select(sched_dep_time, sched_hour, sched_minute) |&gt;\n  head()\n\n# A tibble: 6 × 3\n  sched_dep_time sched_hour sched_minute\n           &lt;int&gt;      &lt;dbl&gt;        &lt;dbl&gt;\n1            515          5           15\n2            529          5           29\n3            540          5           40\n4            545          5           45\n5            600          6            0\n6            558          5           58\n\n\n\n\nNumeric transformations\nTo handle wide ranges of values, apply log transformations. log2() and log10() are easier to interpret; for example, a difference of 1 on the log2 scale corresponds to doubling or halving the original value. Rounding functions include round() (which uses banker’s rounding), floor() and ceiling(). To round to arbitrary multiples, scale the vector, round, and then scale back. Use cut() to convert a continuous variable into categories by specifying break points and optional labels.\n\nx = c(1, 2, 5, 10, 15, 20)\ncut(x, breaks = c(0, 5, 10, 20), labels = c(\"small\", \"medium\", \"large\"))\n\n[1] small  small  small  medium large  large \nLevels: small medium large\n\n\n\n\nNumeric summaries\nSummarizing numeric vectors involves measures of center and spread. The mean is sensitive to extreme values, while the median is more robust. The median daily departure delay is always smaller than the mean because flights can be hours late but rarely leave hours early. Quantiles generalize the median; for example, the 95th percentile ignores the most extreme 5% of values. Measures of spread include the standard deviation and the interquartile range (IQR), the difference between the 75th and 25th percentiles.\n\nflights |&gt;\n  summarise(\n    mean_arr_delay    = mean(arr_delay, na.rm = TRUE),\n    median_arr_delay  = median(arr_delay, na.rm = TRUE),\n    q95_arr_delay     = quantile(arr_delay, 0.95, na.rm = TRUE),\n    sd_arr_delay      = sd(arr_delay, na.rm = TRUE),\n    iqr_arr_delay     = IQR(arr_delay, na.rm = TRUE)\n  )\n\n# A tibble: 1 × 5\n  mean_arr_delay median_arr_delay q95_arr_delay sd_arr_delay iqr_arr_delay\n           &lt;dbl&gt;            &lt;dbl&gt;         &lt;dbl&gt;        &lt;dbl&gt;         &lt;dbl&gt;\n1           6.90               -5            91         44.6            31\n\n\n\n\nKey take‑aways\n\nLogical vectors arise from comparisons and contain only TRUE, FALSE and NA. Combine conditions with Boolean operators and handle missing values carefully.\nSummarize logical vectors with any(), all(), sum() and mean() to answer questions like “Were all flights on time?” or “What fraction of flights left within an hour?”.\nBe mindful of type when working with numbers. Parse numeric strings properly and avoid unintended recycling in arithmetic and comparisons.\nUse numeric transformations—element‑wise minima/maxima, modular arithmetic, logs, rounding and binning—to prepare variables for analysis.\nChoose appropriate summaries: mean vs. median for center, quantiles for tails, and standard deviation or IQR for spread.",
    "crumbs": [
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Session 8 – Variable types I: logical & numeric vectors</span>"
    ]
  },
  {
    "objectID": "09.html",
    "href": "09.html",
    "title": "9  Session 9 – Variable types II: strings & regular expressions",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 – Variable types II: strings & regular expressions</span>"
    ]
  },
  {
    "objectID": "09.html#objectives",
    "href": "09.html#objectives",
    "title": "9  Session 9 – Variable types II: strings & regular expressions",
    "section": "",
    "text": "Create and manipulate strings. Recall that strings are sequences of characters defined by quotes. In R you can use single ' or double \" quotes to create strings. The R4DS text recommends using double quotes by default and switching to single quotes only when the string itself contains quotes. You’ll learn how to escape special characters like backslashes and newlines, how to measure string length with str_length(), and how to extract or replace substrings with str_sub().\nCombine and format strings. Concatenate strings using str_c() (stringr’s equivalent to paste0()), and build templated strings with str_glue(). Understand how these functions handle missing values and learn to collapse vectors of strings with str_flatten().\nUnderstand regular expressions. A regular expression (regex) is a concise language for describing patterns within strings. You’ll learn the difference between literal characters and metacharacters, how quantifiers like ?, * and + control repetition, and how to use character classes ([...]), alternation (|) and anchors (^, $).\nMatch, extract and replace patterns. Use str_detect() to test whether strings match a regex, str_count() to count the number of matches, str_extract() (and str_extract_all()) to pull out matching substrings, str_replace()/str_replace_all() to substitute patterns, and str_split() to separate strings at delimiters. Pair these functions with dplyr verbs to filter, mutate or summarize character variables.\nTidy multiple variables encoded in one string. Recognise when a single string stores several pieces of information and use stringr plus tidyr functions such as str_split() with unnest_longer() or separate() to unpack them. This sets the stage for the next session on factors.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 – Variable types II: strings & regular expressions</span>"
    ]
  },
  {
    "objectID": "09.html#notes",
    "href": "09.html#notes",
    "title": "9  Session 9 – Variable types II: strings & regular expressions",
    "section": "Notes",
    "text": "Notes\n\nCreating strings: quotes, escapes & length\nStrings can be created with single or double quotes. Double quotes are preferred by convention—use single quotes only when the string itself contains double quotes to avoid escaping. Special characters (backslash, newline, tab) require escaping with \\, \\n or \\t. For example:\n\nlibrary(tidyverse)\nlibrary(stringr)\n\n# A string containing both single and double quotes\nquote_example =  \"He said, 'R is great!'\"\nquote_example\n\n[1] \"He said, 'R is great!'\"\n\n# A string with a backslash and a newline\npath =  \"C:\\\\Users\\\\Data\\\\mydata\\n\"\npath\n\n[1] \"C:\\\\Users\\\\Data\\\\mydata\\n\"\n\n\nThe length of a string (number of characters) is computed with str_length(). Note that str_length() counts human‑visible characters, not bytes or graphemes—R treats each character as one unit regardless of encoding:\n\nstr_length(c(\"\", \"abc\", \"😊\"))\n\n[1] 0 3 1\n\n\nUse str_sub() to extract or replace substrings by position. Positive indices count from the start and negative indices count from the end. The function is vectorised, so you can operate on a vector of strings at once:\n\nfruit =  c(\"banana\", \"apple\", \"pear\")\nstr_sub(fruit, 1, 3)      # first three letters\n\n[1] \"ban\" \"app\" \"pea\"\n\nstr_sub(fruit, -3, -1)    # last three letters\n\n[1] \"ana\" \"ple\" \"ear\"\n\nstr_sub(fruit, 1, 1) =  str_to_upper(str_sub(fruit, 1, 1))  # capitalize first letter\nfruit\n\n[1] \"Banana\" \"Apple\"  \"Pear\"  \n\n\n\n\nCombining and formatting strings\nstr_c() concatenates multiple strings together and returns a character vector. It’s similar to paste0() but has a more consistent interface and works seamlessly with dplyr verbs. You can supply a sep argument to insert a separator and a collapse argument to collapse the result into a single string:\n\nspecies =  c(\"Adelie\", \"Chinstrap\", \"Gentoo\")\nn =  c(3, 2, 1)\nstr_c(n, species, sep = \" \", collapse = \"; \")\n\n[1] \"3 Adelie; 2 Chinstrap; 1 Gentoo\"\n\n\nWhen concatenating across rows in a tibble, wrap str_c() inside mutate(). If any of the components are NA, the result will be NA. Use coalesce() to replace missing values with an empty string before concatenation:\n\nlibrary(dplyr)\ndf =  tibble(first = c(\"John\", NA, \"Jenny\"),\n             last  = c(\"Smith\", \"Nguyen\", NA))\ndf |&gt; mutate(full = str_c(coalesce(first, \"\"), coalesce(last, \"\"), sep = \" \"))\n\n# A tibble: 3 × 3\n  first last   full        \n  &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       \n1 John  Smith  \"John Smith\"\n2 &lt;NA&gt;  Nguyen \" Nguyen\"   \n3 Jenny &lt;NA&gt;   \"Jenny \"    \n\n\nstr_glue() from the glue package (loaded automatically with tidyverse) creates templated strings. Inside curly braces {}, you can insert arbitrary R expressions. Missing values are converted to the string “NA”:\n\nlibrary(glue)\ndf |&gt; mutate(message = str_glue(\"Hello {first} {last}!\"))\n\n# A tibble: 3 × 3\n  first last   message          \n  &lt;chr&gt; &lt;chr&gt;  &lt;glue&gt;           \n1 John  Smith  Hello John Smith!\n2 &lt;NA&gt;  Nguyen Hello NA Nguyen! \n3 Jenny &lt;NA&gt;   Hello Jenny NA!  \n\n\nIf you need to combine many strings into one, use str_flatten() rather than repeatedly adding separators. It collapses a character vector into a single string with a given delimiter:\n\nletters =  c(\"a\", \"b\", \"c\")\nstr_flatten(letters, collapse = \", \")\n\n[1] \"a, b, c\"\n\n\n\n\nRegular expressions: pattern basics\nA regular expression (regex) is a concise language for describing sets of strings. In regex, most letters and numbers match themselves (are literals), but certain characters have special meaning (metacharacters). Important metacharacters include:\n\n. matches any character except a newline.\nQuantifiers control repetition: ? matches zero or one occurrence; * matches zero or more; + matches one or more.\nCharacter classes like [aeiou] match any of the enclosed characters. A caret (^) at the start of a class negates it: [^0-9] matches any non‑digit.\nAlternation | matches one of several possibilities; for example, apple|pear matches either word.\nAnchors ^ and $ match the start and end of a string, respectively.\n\nUse regex() to build regexes with settings such as case‑insensitive matching (regex(pattern, ignore_case = TRUE)). The stringr package provides helper functions like str_view() and str_view_all() to visualize matches, but we’ll focus on functions that return useful vectors.\n\n\nDetecting, counting and extracting patterns\nstr_detect() tests whether each element of a character vector matches a regex and returns a logical vector. This pairs naturally with filter() to select rows matching a pattern. str_count() counts the number of times a pattern appears within each string. str_extract() returns the first match, and str_extract_all() returns all matches as a list. str_replace() and str_replace_all() replace the first or all matches with a new string. Finally, str_split() splits strings at matches of a regex and returns a list of pieces.\nLet’s illustrate with flight numbers from the nycflights13 dataset. Airlines in the United States identify a flight by a two‑letter carrier code followed by a flight number. We can build a flight ID string and then extract the numeric part:\n\nlibrary(nycflights13)\nflights_small =  flights |&gt; select(carrier, flight) |&gt; sample_n(5)\n\n# Combine carrier and flight into a single string\nflights_small =  flights_small |&gt; mutate(flight_id = str_c(carrier, flight, sep = \"-\"))\n\n# Detect whether the flight number has exactly 3 digits\nflights_small |&gt; mutate(is_three_digit = str_detect(flight_id, \"-\\\\d{3}$\"))\n\n# A tibble: 5 × 4\n  carrier flight flight_id is_three_digit\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;     &lt;lgl&gt;         \n1 UA         497 UA-497    TRUE          \n2 EV        4326 EV-4326   FALSE         \n3 UA         255 UA-255    TRUE          \n4 9E        3395 9E-3395   FALSE         \n5 DL        2546 DL-2546   FALSE         \n\n# Extract the numeric part of the flight ID\nflights_small |&gt; mutate(number_only = str_extract(flight_id, \"\\\\d+\"))\n\n# A tibble: 5 × 4\n  carrier flight flight_id number_only\n  &lt;chr&gt;    &lt;int&gt; &lt;chr&gt;     &lt;chr&gt;      \n1 UA         497 UA-497    497        \n2 EV        4326 EV-4326   4326       \n3 UA         255 UA-255    255        \n4 9E        3395 9E-3395   9          \n5 DL        2546 DL-2546   2546       \n\n\nNotice that \\d in a regex matches any digit (short for [0-9]), so \\d+ extracts one or more consecutive digits. We had to escape the backslash in the string to make sure it reached the regular‑expression engine.\nstr_replace_all() lets you perform substitutions. For example, you can strip all non‑digit characters from a string using the negated class [^0-9]:\n\nmessy =  c(\"(202) 555-0198\", \"+1-303-555-1212\")\nstr_replace_all(messy, \"[^0-9]\", \"\")\n\n[1] \"2025550198\"  \"13035551212\"\n\n\nFinally, str_split() (or str_split_fixed()) breaks a string into pieces at matches of a pattern. If you know your delimiter, you can also use tidyr’s separate() or separate_wider_delim() to split a column into multiple variables. For example, suppose we have a vector of names in “LAST, First Middle” format:\n\nnames =  c(\"SMITH, John A.\", \"O'NEILL, Anne\", \"Lee, Chen\")\n# Split at the comma and whitespace\ndf_names =  tibble(full = names) |&gt; separate(full, into = c(\"last\", \"rest\"), sep = \",\\\\s*\")\ndf_names\n\n# A tibble: 3 × 2\n  last    rest   \n  &lt;chr&gt;   &lt;chr&gt;  \n1 SMITH   John A.\n2 O'NEILL Anne   \n3 Lee     Chen   \n\n\n\n\nWorking with messy strings in tibbles\nCharacter columns often encode multiple variables in one field. To tidy them, combine stringr and tidyr. As an example, consider the built‑in who2 dataset where column names like sp_m_014 describe the diagnosis, gender and age group. We can pivot the data longer and then separate the compound names into distinct variables:\n\nlibrary(tidyr)\nwho2_long =  who2 |&gt; pivot_longer(\n  cols = !(country:year),\n  names_to = c(\"diagnosis\", \"gender\", \"age\"),\n  names_sep = \"_\",\n  values_to = \"count\"\n)\nwho2_long |&gt; head()\n\n# A tibble: 6 × 6\n  country      year diagnosis gender age   count\n  &lt;chr&gt;       &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;chr&gt; &lt;dbl&gt;\n1 Afghanistan  1980 sp        m      014      NA\n2 Afghanistan  1980 sp        m      1524     NA\n3 Afghanistan  1980 sp        m      2534     NA\n4 Afghanistan  1980 sp        m      3544     NA\n5 Afghanistan  1980 sp        m      4554     NA\n6 Afghanistan  1980 sp        m      5564     NA\n\n\n\n\nKey take‑aways\n\nStrings are enclosed in quotes; use double quotes by default and escape special characters like backslash (\\\\), newline (\\n) or tab (\\t). str_length() and str_sub() let you measure and manipulate substrings.\nConcatenate strings with str_c() and build templated strings with str_glue(). Use coalesce() to handle missing values and str_flatten() to collapse many strings into one.\nRegular expressions describe patterns. Learn the roles of metacharacters (., ?, *, +), character classes ([...]) and alternation (|), and how to anchor patterns at the start (^) or end ($) of a string.\nUse str_detect() to filter strings, str_count() to count occurrences, str_extract() to pull out matches, and str_replace()/str_replace_all() to perform substitutions. These functions are vectorised and integrate well with dplyr.\nTidy multiple variables encoded in one string by combining stringr with tidyr’s separate() and pivot_longer(). This prepares your data for converting character vectors to factors in the next session.",
    "crumbs": [
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Session 9 – Variable types II: strings & regular expressions</span>"
    ]
  },
  {
    "objectID": "10.html",
    "href": "10.html",
    "title": "10  Session 10 – Variable types III: factors",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Session 10 – Variable types III: factors</span>"
    ]
  },
  {
    "objectID": "10.html#objectives",
    "href": "10.html#objectives",
    "title": "10  Session 10 – Variable types III: factors",
    "section": "",
    "text": "Understand why factors exist. Factors represent categorical variables with a fixed and known set of possible values. They prevent accidental typos and give you control over the order in which categories appear in summaries and plots. Learn why treating categories as strings can lead to problems and how factors solve those problems.\nCreate and inspect factors. Use factor() or forcats::fct() to convert character vectors to factors, specify levels and handle invalid values. Learn to inspect and summarize factor levels with levels() and count().\nReorder factor levels for better visualization. Use fct_reorder() to order levels by the values of another variable, fct_relevel() to manually move levels to the front and fct_reorder2() when ordering line‑plot legends. Explore fct_infreq() and fct_rev() to arrange levels by frequency.\nModify and collapse levels. Relabel categories with fct_recode(), combine multiple levels with fct_collapse() and lump rare categories into “other” with the fct_lump_*() family.\nWork with ordered factors. Create ordered factors for ordinal data (e.g., satisfaction ratings) and appreciate when an intrinsic order exists.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Session 10 – Variable types III: factors</span>"
    ]
  },
  {
    "objectID": "10.html#notes",
    "href": "10.html#notes",
    "title": "10  Session 10 – Variable types III: factors",
    "section": "Notes",
    "text": "Notes\n\nWhy factors?\nIn data analysis, categorical variables often have a limited set of allowed values (e.g., months, continents, income bands). Storing them as plain strings introduces two common issues: you can type invalid values and the default alphabetical sorting is rarely meaningful. A factor solves these problems by enforcing a list of valid levels and providing explicit control over their order. For example, the months “Jan”, “Feb”, … “Dec” should appear in calendar order, not alphabetical. Converting a character vector of month names to a factor with those 12 levels ensures that invalid spellings become missing values (or an error if you use forcats::fct()), and sorting follows the chosen level order.\nFactors are also necessary for plotting. When you map a factor to the y‑axis of a bar chart or to color/shape, ggplot2 displays the categories in the order given by the factor. If you leave your categorical variable as a character, ggplot2 silently converts it to a factor using the alphabetical order.\n\n\nCreating factors\nBase R provides a factor() function. You pass a character vector and optionally a vector of levels:\n\nlibrary(tidyverse)\n\nmonths_raw  = c(\"Dec\", \"Apr\", \"Jan\", \"Mar\")\nmonth_levels = c(\"Jan\", \"Feb\", \"Mar\", \"Apr\", \"May\", \"Jun\",\n                 \"Jul\", \"Aug\", \"Sep\", \"Oct\", \"Nov\", \"Dec\")\n\n# Create a factor with explicit levels\nmonths_factor = factor(months_raw, levels = month_levels)\nmonths_factor\n\n[1] Dec Apr Jan Mar\nLevels: Jan Feb Mar Apr May Jun Jul Aug Sep Oct Nov Dec\n\n\nIf a value does not appear in levels, factor() converts it to NA. The forcats package (loaded with the tidyverse) provides fct() which raises an error if you try to create a factor from values not present in levels, helping you catch typos earlier.\nYou can check the current levels with levels(months_factor) and count the number of observations in each category using count():\n\nlibrary(forcats)\n\nlevels(months_factor)  # returns the vector of valid levels\n\n [1] \"Jan\" \"Feb\" \"Mar\" \"Apr\" \"May\" \"Jun\" \"Jul\" \"Aug\" \"Sep\" \"Oct\" \"Nov\" \"Dec\"\n\n# Example using the gss_cat dataset from forcats\ngss_cat |&gt; count(race)\n\n# A tibble: 3 × 2\n  race      n\n  &lt;fct&gt; &lt;int&gt;\n1 Other  1959\n2 Black  3129\n3 White 16395\n\n\nThe gss_cat dataset (part of forcats) contains multiple factor variables such as race, marital, rincome and partyid. Use it to practice summarizing and visualizing categorical data.\n\n\nReordering levels\nSometimes the alphabetical or default order of factor levels is arbitrary. Reordering levels improves the interpretability of plots. The fct_reorder() function takes three arguments: the factor to reorder, a numeric vector whose values determine the order, and optionally a function to combine multiple values.\n\n# Average TV hours by religion\nrelig_summary = gss_cat |&gt;\n  group_by(relig) |&gt;\n  summarise(tvhours = mean(tvhours, na.rm = TRUE), .groups = \"drop\")\n\n# Plot without reordering\nggplot(relig_summary, aes(x = tvhours, y = relig)) +\n  geom_point()\n\n\n\n\n\n\n\n# Plot with levels reordered by tvhours\nggplot(relig_summary, aes(x = tvhours, y = fct_reorder(relig, tvhours))) +\n  geom_point()\n\n\n\n\n\n\n\n\nHere, reordering the relig factor makes it clear that people in the “Don’t know” category watch more TV than those in “Other Eastern” religions. Use fct_relevel() when you need to manually move one or more levels to the front—for example, to emphasize a special category:\n\n# Move \"Not applicable\" to the front in rincome\nrincome_summary = gss_cat |&gt;\n  group_by(rincome) |&gt;\n  summarise(age = mean(age, na.rm = TRUE), .groups = \"drop\")\n\nggplot(rincome_summary, aes(x = age, y = fct_relevel(rincome, \"Not applicable\"))) +\n  geom_point()\n\n\n\n\n\n\n\n\nFor line plots with many categories, fct_reorder2() orders the legend by the y‑value at the largest x‑value, making it easier to match the lines to the legend.\nFinally, fct_infreq() orders levels by their frequency, and fct_rev() reverses that order. This combination is useful for bar charts where you want categories sorted from least to most common.\n\n\nModifying levels\nReordering controls the order of the levels, but sometimes you want to change the labels or combine categories. The fct_recode() function renames levels by providing the new name on the left and the old name on the right:\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_recode(partyid,\n      \"Republican, strong\"    = \"Strong republican\",\n      \"Republican, weak\"      = \"Not str republican\",\n      \"Independent, near rep\" = \"Ind,near rep\",\n      \"Independent, near dem\" = \"Ind,near dem\",\n      \"Democrat, weak\"        = \"Not str democrat\",\n      \"Democrat, strong\"      = \"Strong democrat\"\n    )\n  ) |&gt;\n  count(partyid)\n\n# A tibble: 10 × 2\n   partyid                   n\n   &lt;fct&gt;                 &lt;int&gt;\n 1 No answer               154\n 2 Don't know                1\n 3 Other party             393\n 4 Republican, strong     2314\n 5 Republican, weak       3032\n 6 Independent, near rep  1791\n 7 Independent            4119\n 8 Independent, near dem  2499\n 9 Democrat, weak         3690\n10 Democrat, strong       3490\n\n\nTo collapse multiple categories into a smaller set, use fct_collapse(). Supply the new category names and a vector of existing levels to combine:\n\ngss_cat |&gt;\n  mutate(\n    partyid = fct_collapse(partyid,\n      other = c(\"No answer\", \"Don't know\", \"Other party\"),\n      rep   = c(\"Strong republican\", \"Not str republican\"),\n      ind   = c(\"Ind,near rep\", \"Independent\", \"Ind,near dem\"),\n      dem   = c(\"Not str democrat\", \"Strong democrat\")\n    )\n  ) |&gt;\n  count(partyid)\n\n# A tibble: 4 × 2\n  partyid     n\n  &lt;fct&gt;   &lt;int&gt;\n1 other     548\n2 rep      5346\n3 ind      8409\n4 dem      7180\n\n\nWhen you need to simplify a factor with many rare categories, the fct_lump_*() family automatically groups the smallest categories into “Other”. For example, fct_lump_n(factor, n = 10) keeps the 10 most common categories and lumps the rest:\n\ngss_cat |&gt;\n  mutate(relig = fct_lump_n(relig, n = 10)) |&gt;\n  count(relig, sort = TRUE)\n\n# A tibble: 10 × 2\n   relig                       n\n   &lt;fct&gt;                   &lt;int&gt;\n 1 Protestant              10846\n 2 Catholic                 5124\n 3 None                     3523\n 4 Christian                 689\n 5 Other                     458\n 6 Jewish                    388\n 7 Buddhism                  147\n 8 Inter-nondenominational   109\n 9 Moslem/islam              104\n10 Orthodox-christian         95\n\n\n\n\nOrdered factors\nSome categories have an intrinsic order (e.g., “low”, “medium”, “high” or rating scales from 1 to 5). You can create an ordered factor by setting ordered = TRUE in factor() or using forcats::fct() with the ordered argument. Ordered factors behave like numeric variables for certain operations (e.g., min() and max() work) but remain categorical. Always think carefully about whether your categories truly have a natural order before imposing one.\n\n\nKey take‑aways\n\nFactors represent categorical variables with a fixed set of levels and control the order of categories.\nCreating a factor from a string requires specifying the valid levels; values not in the list become NA or throw an error.\nInspect factor levels with levels() and summarise them with count().\nUse the forcats functions fct_reorder(), fct_relevel() and fct_reorder2() to rearrange levels for clearer plots.\nModify factor labels with fct_recode(), collapse levels with fct_collapse() and lump rare categories with fct_lump_n().\nOrdered factors capture ordinal scales—set ordered = TRUE when the categories have a meaningful ranking.",
    "crumbs": [
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Session 10 – Variable types III: factors</span>"
    ]
  },
  {
    "objectID": "11.html",
    "href": "11.html",
    "title": "11  Session 11 – Variable types IV: dates & times",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 – Variable types IV: dates & times</span>"
    ]
  },
  {
    "objectID": "11.html#objectives",
    "href": "11.html#objectives",
    "title": "11  Session 11 – Variable types IV: dates & times",
    "section": "",
    "text": "Understand different date/time types. Describe the difference between dates (&lt;date&gt;), times (&lt;time&gt;) and date‑times (&lt;dttm&gt;) in R. Emphasize using the simplest type that meets the analysis requirements.\nCreate date/time objects. Learn how to parse strings into dates with lubridate helpers such as ymd(), mdy(), dmy() etc., and how to assemble dates and date‑times from separate components with make_date() and make_datetime().\nExtract and modify components. Use accessor functions year(), month(), mday(), yday(), wday(), hour(), minute() and second() to pull out parts of a date‑time. Learn to change components in place or with update().\nRound dates and compute spans. Round dates to a unit (e.g., week, month) with floor_date(), round_date() and ceiling_date(). Understand time spans and the difference between durations (exact seconds), periods (human units like weeks and months) and intervals.\nRecognize time‑zone issues. Appreciate that date‑time values always have an associated time zone. Learn how to specify and change the time zone when parsing date‑times.",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 – Variable types IV: dates & times</span>"
    ]
  },
  {
    "objectID": "11.html#notes",
    "href": "11.html#notes",
    "title": "11  Session 11 – Variable types IV: dates & times",
    "section": "Notes",
    "text": "Notes\n\nWhy dates and times?\nDates and times are deceptively simple. They appear ubiquitous and straightforward, but the more you work with them, the more quirks emerge. A year isn’t always 365 days, not every day has exactly 24 hours and occasionally a minute may have 61 seconds. These complexities arise because calendar units reconcile astronomical cycles (earth’s rotation and orbit) with human conventions (leap years, daylight saving time). To manage them, R doesn’t treat dates as plain strings but provides special classes and functions.\nIn the tidyverse, lubridate simplifies working with date and time data. It isn’t loaded by default with the tidyverse, so you must call library(lubridate).\n\n\nCreating date/times\nThere are three common ways to create date/time objects:\n\nFrom strings. Use lubridate helpers to parse strings that contain dates or date‑times. Arrange the letters “y”, “m” and “d” to match the order of year, month and day in the string, and lubridate will infer the format. For example:\n\n\nlibrary(tidyverse)\nlibrary(lubridate)\n\nymd(\"2025-02-18\")        # ISO format year-month-day\n\n[1] \"2025-02-18\"\n\nmdy(\"January 31, 2025\")  # month-day-year with month name\n\n[1] \"2025-01-31\"\n\ndmy(\"31-01-2025\")        # day-month-year\n\n[1] \"2025-01-31\"\n\n# parse date‑times by adding h, m and s\nymd_hms(\"2025-02-18 14:30:15\")\n\n[1] \"2025-02-18 14:30:15 UTC\"\n\nmdy_hm(\"2/18/2025 2:30 pm\")  # 12‑hour clock with am/pm\n\n[1] \"2025-02-18 14:30:00 UTC\"\n\n\nThese functions also accept numeric input without quotes (e.g., ymd(20250218)). You can supply a tz argument to set the time zone.\n\nFrom individual components. When the year, month, day (and optionally hour, minute) live in separate columns, combine them with make_date() or make_datetime(). For example:\n\n\ndf = tibble(year = c(2023, 2023),\n            month = c(5, 10),\n            day = c(14, 1),\n            hour = c(9, 16),\n            minute = c(30, 0))\n\n# Create new columns without using the pipe to avoid confusion\ndf$date = make_date(df$year, df$month, df$day)\ndf$datetime = make_datetime(df$year, df$month, df$day, df$hour, df$minute)\ndf\n\n# A tibble: 2 × 7\n   year month   day  hour minute date       datetime           \n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;  &lt;dbl&gt; &lt;date&gt;     &lt;dttm&gt;             \n1  2023     5    14     9     30 2023-05-14 2023-05-14 09:30:00\n2  2023    10     1    16      0 2023-10-01 2023-10-01 16:00:00\n\n\n\nFrom existing date/time objects. Use functions like as_datetime() to coerce between date and date‑time classes or specify a time zone when converting.\n\n\n\nExtracting components\nOnce you have a date/time, you can access its components. Use year(), month(), mday() (day of month), yday() (day of year), wday() (day of week), hour(), minute() and second():\n\ndt = ymd_hms(\"2025-02-18 14:30:15\", tz = \"America/Chicago\")\n\nyear(dt)         # 2025\n\n[1] 2025\n\nmonth(dt)        # 2\n\n[1] 2\n\nmonth(dt, label = TRUE)   # Feb\n\n[1] Feb\n12 Levels: Jan &lt; Feb &lt; Mar &lt; Apr &lt; May &lt; Jun &lt; Jul &lt; Aug &lt; Sep &lt; ... &lt; Dec\n\nwday(dt, label = TRUE, abbr = FALSE)  # Tuesday\n\n[1] Tuesday\n7 Levels: Sunday &lt; Monday &lt; Tuesday &lt; Wednesday &lt; Thursday &lt; ... &lt; Saturday\n\nhour(dt)         # 14\n\n[1] 14\n\nminute(dt)       # 30\n\n[1] 30\n\nsecond(dt)       # 15\n\n[1] 15\n\n\nThese accessors are vectorised and return integers or ordered factors. You can also modify components by assigning (year(dt) = 2026) or with update():\n\ndt2 = update(dt, year = 2026, month = 1, mday = 1, hour = 0)\ndt2  # \"2026-01-01 00:30:15 CST\"\n\n[1] \"2026-01-01 00:30:15 CST\"\n\n\nIf values overflow the usual range (e.g., day = 30 in February), lubridate automatically rolls over to the next month.\n\n\nRounding and time spans\nTo simplify a continuous timeline into regular intervals, round dates and date‑times. floor_date(x, \"week\") rounds down to the start of the week, ceiling_date(x, \"month\") rounds up to the next month and round_date(x, \"day\") rounds to the nearest day. For example, to count weekly observations you can round each date to the start of the week and then tally the number of observations per week.\nArithmetic with date‑times yields time spans. Subtracting two dates returns a difftime, which stores a difference in days, hours, etc. lubridate defines three classes: durations (exact seconds), periods (human units like months) and intervals (a start and end point). Use constructors like ddays(), dhours() and dminutes() to create durations.\n\nage = today() - ymd(\"19900101\")  # difftime\nas.duration(age)                 # duration in seconds\n\n[1] \"1130284800s (~35.82 years)\"\n\n# add one month (a period)\ntoday() + months(1)\n\n[1] \"2025-11-26\"\n\n# difference between dates as period\nperiod = as.period(ymd(\"2025-03-01\") - ymd(\"2025-01-15\"))\nperiod\n\n[1] \"45d 0H 0M 0S\"\n\n\n\n\nTime zones\nDate‑times always carry a time zone. If you omit the time zone, R defaults to your system’s time zone. When parsing strings, supply tz to ensure correct interpretation. Use with_tz() to change how an instant is displayed in another time zone, and force_tz() to change the underlying instant (rarely needed). Avoid combining date‑times with unspecified time zones.\n\n\nKey take‑aways\n\nDates and date‑times are special data types; use the simplest type that serves your purpose.\nUse lubridate helpers (ymd(), ymd_hms() etc.) or make_date()/make_datetime() to create date/time objects.\nExtract components with year(), month(), wday() and friends, and modify them with assignment or update().\nRound dates to meaningful units using floor_date(), ceiling_date() and round_date().\nDistinguish between durations, periods and intervals when doing arithmetic with dates.\nAlways be conscious of time zones; specify tz explicitly when parsing date‑times.\n\n\n\n\n\n\n\n\n–&gt;  –&gt;",
    "crumbs": [
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Session 11 – Variable types IV: dates & times</span>"
    ]
  },
  {
    "objectID": "12.html",
    "href": "12.html",
    "title": "12  Session 12 – Missing values & data cleaning",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 – Missing values & data cleaning</span>"
    ]
  },
  {
    "objectID": "12.html#objectives",
    "href": "12.html#objectives",
    "title": "12  Session 12 – Missing values & data cleaning",
    "section": "",
    "text": "Differentiate explicit and implicit missing values. Explicitly missing values appear in your data as NA, whereas implicitly missing values arise when an entire observation is absent. Recognizing the difference – “the presence of an absence” versus “the absence of a presence” – guides how you handle them.\nUnderstand the infectious nature of NA. Missing values propagate through calculations and comparisons. The online book revisits their infectious nature and discusses how to check for missing values before summarizing or visualizing data.\nHandle explicit missing values. Learn how to carry values forward with tidyr::fill() (last observation carried forward), replace missing values with fixed values using dplyr::coalesce(), and convert special codes (e.g., -99) to NA with dplyr::na_if().\nReveal and fill implicit missing values. Use tidyr::pivot_wider() to make implicit missings explicit and tidyr::complete() to generate all combinations of variables so that missing combinations appear as rows. Understand when to drop structurally missing values by setting values_drop_na = TRUE.\nDeal with empty groups. Recognize that groups with no observations (empty groups) are a form of missingness and preserve them by setting .drop = FALSE in count() or group_by(). Be aware of how summary functions behave on zero‑length vectors.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 – Missing values & data cleaning</span>"
    ]
  },
  {
    "objectID": "12.html#notes",
    "href": "12.html#notes",
    "title": "12  Session 12 – Missing values & data cleaning",
    "section": "Notes",
    "text": "Notes\n\nWhy worry about missing values?\nMissing data is ubiquitous. Sometimes a value is unknown, it wasn’t recorded or it simply doesn’t exist. R represents absent values with the special marker NA, and these NAs are infectious: any arithmetic or logical operation involving a missing value yields a missing result. This means you must consciously handle missing values before drawing conclusions. The online book revisits missing values after we have encountered them in plotting and transformation and emphasizes their two forms—explicit and implicit. An explicit missing value is the presence of an absence (an NA in a cell), while an implicit missing value is the absence of a presence (a row that should exist but doesn’t).\n\n\nTools for explicit missing values\nWhen you see NAs in your data, decide whether to propagate, fill or replace them:\n\nCarrying values forward/backward. In hand-entered data, a missing value may mean “same as the previous value”. The fill() function from tidyr takes one or more columns and fills missing values with the most recent non‑missing value (last observation carried forward, or LOCF). For example, if a dataset records a patient’s treatment at visits 1-3 but omits the patient’s name in visits 2 and 3, fill() will copy the name down.\n\nlibrary(tidyverse)\n\ntreatment = tribble(\n  ~person,           ~treatment, ~response,\n  \"Derrick Whitmore\", 1,         7,\n  NA,                 2,         10,\n  NA,                 3,         NA,\n  \"Katherine Burke\",  1,         4\n)\n# Fill missing person names forward\ntreatment |&gt; fill(person)\n\n# A tibble: 4 × 3\n  person           treatment response\n  &lt;chr&gt;                &lt;dbl&gt;    &lt;dbl&gt;\n1 Derrick Whitmore         1        7\n2 Derrick Whitmore         2       10\n3 Derrick Whitmore         3       NA\n4 Katherine Burke          1        4\n\n\nReplacing with fixed values. If missing values represent a fixed value-such as zero or “unknown”—use coalesce() to replace NA with that value. coalesce() returns the first non‑missing value among its arguments and is vectorised across rows. It is also handy when combining multiple columns: coalesce(a, b, c) picks a if it’s non‑missing, otherwise b, then c.\n\nx = c(1, 4, 5, 7, NA)\ncoalesce(x, 0)        # replace NA with zero\n\n[1] 1 4 5 7 0\n\n# Combining two columns: take value from x if present, else from y\ndf = tibble(x = c(2, NA, 5), y = c(1, 3, NA))\ndf |&gt; mutate(value = coalesce(x, y))\n\n# A tibble: 3 × 3\n      x     y value\n  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1     2     1     2\n2    NA     3     3\n3     5    NA     5\n\n\nConverting sentinel values to NA. Older software sometimes encodes missing values with special numbers like 99 or -999. Use na_if(x, sentinel) to replace those sentinel values with NA. For instance, if a dataset records missing ages as -99, call na_if(age, -99) before analyzing.\n\n\n\nImplicit missing values and how to reveal them\nImplicit missing values occur when some combinations of variables are absent. For instance, suppose you record the price of a stock each quarter but forget to enter the first quarter in 2021. That observation is implicitly missing. Two tidyverse tools help reveal implicit missings:\n\nPivoting. Pivoting data wider with pivot_wider() can transform implicit missings into explicit ones because every combination of rows and new columns must have a value. Conversely, pivoting longer (pivot_longer()) can drop structural missing values when values_drop_na = TRUE.\nCompleting combinations. complete() creates all combinations of the supplied variables and inserts rows with NA where combinations are missing. You can specify explicit ranges for variables (e.g., years = 2019:2021) to ensure the dataset spans the desired range. If your data includes factors, use .drop = FALSE in count() or group_by() to preserve empty groups.\n\nFor example, consider the built‑in msleep dataset from ggplot2, which lists characteristics of mammals. Some combinations of diet category (vore) and conservation status do not occur in the data. You can reveal those missing combinations using complete(vore, conservation) and then replace the implicit missing counts with zero:\n\nlibrary(ggplot2)\nlibrary(tidyr)\n\n# Compute counts by vore and conservation\ncounts = msleep |&gt; count(vore, conservation)\n\n# Make implicit missings explicit and replace NA counts with zero\ncounts_complete = counts |&gt; \n  complete(vore, conservation) |&gt; \n  mutate(n = coalesce(n, 0))\n\ncounts_complete |&gt; arrange(vore, conservation)\n\n# A tibble: 35 × 3\n   vore  conservation     n\n   &lt;chr&gt; &lt;chr&gt;        &lt;dbl&gt;\n 1 carni cd               1\n 2 carni domesticated     2\n 3 carni en               1\n 4 carni lc               5\n 5 carni nt               1\n 6 carni vu               4\n 7 carni &lt;NA&gt;             5\n 8 herbi cd               1\n 9 herbi domesticated     7\n10 herbi en               2\n# ℹ 25 more rows\n\n\n\n\nEmpty groups and zero‑length vectors\nWhen summarizing data by a factor, some levels may have zero observations. These empty groups behave like implicit missing values. Functions such as count() and group_by() drop empty groups by default. To keep them, set .drop = FALSE. Be careful: summary functions applied to empty vectors may return surprising results—for example, mean() of a zero‑length vector returns NaN. Always interpret summaries of empty groups cautiously.\n\n\nKey take‑aways\n\nDistinguish explicit missing values (NA in your data) from implicit missing values (rows that should exist but are absent).\nUse is.na() to detect missing values and remember that NA values propagate through computations.\nFor explicit missings, fill() carries values forward or backward; coalesce() replaces NAs with a fixed value or the first non‑missing value; and na_if() converts sentinel codes to NA.\nFor implicit missings, pivot_wider() and complete() reveal absent combinations; specify .drop = FALSE to preserve empty factor levels.\nWhen summarizing empty groups, interpret results carefully: functions like mean() may return NaN on zero‑length vectors.",
    "crumbs": [
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Session 12 – Missing values & data cleaning</span>"
    ]
  },
  {
    "objectID": "13.html",
    "href": "13.html",
    "title": "13  Session 13 – Relational data",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13 – Relational data</span>"
    ]
  },
  {
    "objectID": "13.html#objectives",
    "href": "13.html#objectives",
    "title": "13  Session 13 – Relational data",
    "section": "",
    "text": "Recognize relational data and keys. A relational dataset organizes information across multiple tables; each table has a primary key that uniquely identifies each row. A foreign key links a row in one table to a row in another table. Learn to identify keys and verify their uniqueness.\nUse mutating joins to combine tables. Mutating joins (e.g., left_join(), inner_join(), right_join(), full_join()) add new columns from a matching table. They match rows by keys and copy across variables. Understand how left join keeps all rows of the left table and how duplicates arise in many-to-many joins.\nSpecify join keys explicitly. The default join behavior uses all common variable names as keys; learn to set keys manually and handle compound keys.\nUse filtering joins to filter rows. Filtering joins (semi_join(), anti_join()) keep rows in one table based on whether a match exists in another table; they never duplicate rows.\nAvoid pitfalls with joins. Recognize issues such as missing matches, duplicate keys and many-to-many relationships that can inflate row counts.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13 – Relational data</span>"
    ]
  },
  {
    "objectID": "13.html#notes",
    "href": "13.html#notes",
    "title": "13  Session 13 – Relational data",
    "section": "Notes",
    "text": "Notes\n\nWhat is relational data?\nData rarely live in a single flat table. When information naturally splits into multiple tables, you have relational data. For instance, a flights database has separate tables describing individual flights, airlines, planes and airports. To connect tables, each has a primary key—a variable (or set of variables) that uniquely identifies each row—and other tables contain foreign keys referencing those primary keys. In the Lahman baseball database, every player has a playerID that links their biographical details in the People table to performance stats in tables like Batting and salary records. Keys allow you to work with smaller, more coherent tables instead of duplicating information across columns.\nTo reliably join tables you must check that the key is unique within the table (no duplicates) and that the key column exists in both tables. Use count() and filter(n &gt; 1) to identify duplicate keys. When a table lacks a single unique key you can use multiple variables as a compound key—for example, the combination of airport and hour uniquely identifies each weather observation in the flights data.\n\n\nMutating joins\nA mutating join combines variables from two tables by matching rows on their keys. There are four common mutating joins, each defined by which rows they keep:\n\nInner join (inner_join(x, y)): keeps only rows where the key appears in both x and y. Rows in x without a match are dropped, and rows in y without a match are ignored. If a key matches multiple rows in y, the row in x is duplicated once for each match.\nLeft join (left_join(x, y)): keeps all rows of x and adds matching columns from y. If there’s no match, the new columns are filled with NA. Left joins are the most common because they allow you to augment your existing data with additional variables while preserving the original observations.\nRight join (right_join(x, y)): keeps all rows of y, matching as many rows in x as possible. Unmatched rows in y have NA for variables from x.\nFull join (full_join(x, y)): keeps all rows from both x and y, filling in NA where there is no match.\n\nMutating joins add columns like mutate(), so if your table has many variables you might not notice the added columns. Use select() before joining to narrow to relevant keys and columns. When you join on keys that are not unique in one or both tables, dplyr will duplicate rows for each match; this can create a many-to-many join that expands your dataset dramatically and triggers a warning.\n\nSpecifying join keys\nBy default, dplyr uses all variables that appear in both tables as the join key. This works if the keys have the same name in both tables but can cause unexpected matches. You can specify the key(s) with the by argument:\n\nlibrary(tidyverse)\nlibrary(nycflights13\n        )\nflights |&gt; \n  left_join(airlines, by = \"carrier\")\n\n# A tibble: 336,776 × 20\n    year month   day dep_time sched_dep_time dep_delay arr_time sched_arr_time\n   &lt;int&gt; &lt;int&gt; &lt;int&gt;    &lt;int&gt;          &lt;int&gt;     &lt;dbl&gt;    &lt;int&gt;          &lt;int&gt;\n 1  2013     1     1      517            515         2      830            819\n 2  2013     1     1      533            529         4      850            830\n 3  2013     1     1      542            540         2      923            850\n 4  2013     1     1      544            545        -1     1004           1022\n 5  2013     1     1      554            600        -6      812            837\n 6  2013     1     1      554            558        -4      740            728\n 7  2013     1     1      555            600        -5      913            854\n 8  2013     1     1      557            600        -3      709            723\n 9  2013     1     1      557            600        -3      838            846\n10  2013     1     1      558            600        -2      753            745\n# ℹ 336,766 more rows\n# ℹ 12 more variables: arr_delay &lt;dbl&gt;, carrier &lt;chr&gt;, flight &lt;int&gt;,\n#   tailnum &lt;chr&gt;, origin &lt;chr&gt;, dest &lt;chr&gt;, air_time &lt;dbl&gt;, distance &lt;dbl&gt;,\n#   hour &lt;dbl&gt;, minute &lt;dbl&gt;, time_hour &lt;dttm&gt;, name &lt;chr&gt;\n\n\nUse join_by() for more complex expressions or to join on multiple columns. When names differ between tables, use named vector syntax such as by = c(\"playerID\" = \"id\").\n\n\n\nFiltering joins\nFiltering joins keep or drop rows from the first table based solely on whether a match exists in the second table. Unlike mutating joins, they never duplicate rows:\n\nSemi join (semi_join(x, y)): keeps rows in x that have at least one match in y. Use it to filter x to cases that appear in another table (e.g., players with salary records).\nAnti join (anti_join(x, y)): keeps rows in x that have no match in y. Use it to find observations in x that are absent from y (e.g., players without recorded salaries).\n\nFiltering joins are useful for diagnosing which observations will be lost in a mutating join or identifying missing data.\n\n\nHandling missing matches and duplicate keys\nMissing matches are inevitable when joining real data. For example, if you left-join plane characteristics to flight records, some planes will have unknown details; the new columns will contain NA. Always consider whether missing matches represent data that do not exist or cases where the foreign key is incorrect.\nDuplicate keys pose a greater problem: when a key is not unique in either table, a join multiplies rows. Check for duplicates beforehand and decide whether to filter, summarise, or use distinct keys. For many-to-many relationships, decide whether you want all combinations (set relationship = \"many-to-many\") or whether you need to pre-aggregate one table to ensure one-to-many relationships.\n\n\nKey take-aways\n\nIdentify primary keys that uniquely identify each row in a table and foreign keys that link to those keys in another table. Compound keys may be necessary when a single column isn’t unique.\nA mutating join adds columns from another table by matching on keys; choose between inner_join(), left_join(), right_join(), and full_join() based on which rows you want to keep.\nA filtering join retains rows in x based on whether they match rows in y and never duplicates rows; use semi_join() and anti_join() to explore overlaps between tables.\nAlways check your keys for uniqueness and consider the implications of missing or duplicate matches. Many-to-many joins can inflate the number of rows dramatically.\nUse select() to keep only necessary columns before joining, and specify join keys explicitly to avoid accidental matches.",
    "crumbs": [
      "<span class='chapter-number'>13</span>  <span class='chapter-title'>Session 13 – Relational data</span>"
    ]
  },
  {
    "objectID": "14.html",
    "href": "14.html",
    "title": "14  Session 14 – Advanced data import I: spreadsheets",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Session 14 – Advanced data import I: spreadsheets</span>"
    ]
  },
  {
    "objectID": "14.html#objectives",
    "href": "14.html#objectives",
    "title": "14  Session 14 – Advanced data import I: spreadsheets",
    "section": "",
    "text": "Recognize why spreadsheets are common and problematic. Data often arrives as Excel or Google Sheets files. Compared to CSVs, spreadsheets can contain multiple worksheets, extraneous formatting and mixed data types. Understand why a clean import requires careful selection of sheets, ranges and column names.\nLoad Excel spreadsheets into R. Use the readxl package to read .xls and .xlsx files with functions such as read_xls(), read_xlsx() and the convenience wrapper read_excel(). Specify column names, skip rows and define custom missing‑value strings when importing.\nWork with multiple worksheets. List the sheet names in a workbook with excel_sheets(), read individual sheets with read_excel() and combine them with bind_rows().\nRead part of a sheet. Use the range argument to select a specific rectangle of cells when a worksheet contains extraneous text above or below the data frame.\nUnderstand type guessing and specify column types. Excel cells can contain booleans, numbers, datetimes or strings; readxl guesses column types but you can override them. Learn how to inspect and set col_types when necessary.\nWrite Excel files. Create spreadsheets using the writexl::write_xlsx() function and appreciate its limitations; learn when to use openxlsx for more advanced formatting.\nLoad data from Google Sheets. Use the googlesheets4 package to read public or shared spreadsheets. read_sheet() works similarly to read_excel(): it accepts a URL or sheet ID and allows you to specify column names, missing values and column types. Understand that many tasks translate directly from Excel to Google Sheets, but minor differences may require adjustments.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Session 14 – Advanced data import I: spreadsheets</span>"
    ]
  },
  {
    "objectID": "14.html#notes",
    "href": "14.html#notes",
    "title": "14  Session 14 – Advanced data import I: spreadsheets",
    "section": "Notes",
    "text": "Notes\n\nWhy spreadsheets?\nPlain‑text formats like CSV are easy to version control and read into R, but many collaborators use spreadsheets to store data. A spreadsheet file can hold multiple worksheets, merge cells for visual layout and apply formatting. This flexibility makes spreadsheets convenient for humans but harder for code. When you receive a workbook, you need to identify which worksheet(s) contain the data, decide if there are header or footer rows to skip, and ensure that each column represents a single variable.\n\n\nReading Excel spreadsheets\nThe readxl package provides three functions to import Excel files: read_xls() for the older .xls format, read_xlsx() for the newer .xlsx format and read_excel(), which automatically detects the file type. All three functions return a tibble and have a consistent interface. The first argument is the path to the file and the optional sheet argument selects a worksheet by name or position. For example, to read a spreadsheet called students.xlsx you would call:\n\nlibrary(tidyverse)\nlibrary(readxl)\n\nstudents = read_excel(\"data/students.xlsx\")\n\nWhen importing, you often need to adjust the column names and missing‑value strings. Provide a character vector to col_names to set variable names explicitly and set skip to ignore extraneous header rows. Use the na argument to tell readxl which strings should be converted to NA. For example, the spreadsheet in Chapter 20 of R for Data Science uses \"N/A\" to denote missing values, so you would write:\n\nstudents = read_excel(\n  \"data/students.xlsx\",\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\")\n)\n\n\n\nWorking with multiple worksheets\nSpreadsheets can contain multiple datasets in separate worksheets. Use excel_sheets() to list the sheet names and then supply a sheet name or index to read_excel(). When a workbook splits a dataset across sheets (e.g. one sheet per site), read each sheet into its own tibble and then combine the tibbles with bind_rows(). For example, suppose penguins.xlsx contains three sheets – Torgersen, Biscoe and Dream islands – each with the same variables:\n\npath = \"data/penguins.xlsx\"\nsheet_names = excel_sheets(path)\n\n# Read each sheet and combine into one tibble\npenguins = sheet_names |&gt; \n  set_names() |&gt;  # preserve the sheet names\n  map(~ read_excel(path, sheet = .x, na = \"NA\")) |&gt; \n  bind_rows()\n\n\n\nReading part of a sheet\nMany spreadsheets mix notes and formatting with data. When the data table occupies only a portion of the worksheet, use the range argument to read a specific rectangle of cells. For example, the deaths.xlsx example in the readxl package has non‑data text above and below the table. The data starts in cell A5 and ends in cell F15, so you would call:\n\ndeaths_path = readxl_example(\"deaths.xlsx\")\ndeaths = read_excel(deaths_path, range = \"A5:F15\")\n\nThis reads only the ten rows of interest.\n\n\nData types and type guessing\nUnlike CSV files, Excel cells can hold booleans, numbers, datetimes or text. When you import a spreadsheet, readxl guesses the column type by scanning a sample of cells. If all values look like numbers, it will treat the column as numeric; if some cells contain dates, the column becomes a datetime; otherwise it becomes a character vector. Mixed types or hidden formatting can lead to surprises. You can always inspect the inferred types and, if necessary, override them with the col_types argument. Recognize that Excel stores dates internally as numbers (days since 1970) and that you may need to convert them to proper date or datetime objects.\n\n\nWriting Excel files\nTo share results with collaborators who use spreadsheets, write data frames back to Excel using the writexl package. The write_xlsx() function accepts a data frame and a path and writes an .xlsx file with simple formatting. For example:\n\nlibrary(writexl)\nbake_sale = tibble(\n  item = factor(c(\"brownie\", \"cupcake\", \"cookie\")),\n  quantity = c(10, 5, 8)\n)\nwrite_xlsx(bake_sale, path = \"bake-sale.xlsx\")\n\nIf you need more control over formatting, writing to multiple worksheets or styling cells, consider the openxlsx package, which provides functions for adjusting column widths, font styles and cell colours.\n\n\nGoogle Sheets\nExcel files live on your computer, but many teams use Google Sheets to collaborate. The googlesheets4 package reads and writes Google Sheets using the Sheets API. Its main function read_sheet() downloads data given a Google Sheets URL or file ID. The syntax parallels read_excel(): you can supply col_names, skip, na and col_types to clean up the import. For example, to read a publicly shared sheet by its ID:\n\nlibrary(googlesheets4)\n\n# Deauthorize if the sheet is publicly accessible\ngs4_deauth()\n\nsheet_id = \"1V1nPp1tzOuutXFLb3G9Eyxi3qxeEhnOXUzL5_BcCQ0w\"\nstudents = read_sheet(\n  sheet_id,\n  col_names = c(\"student_id\", \"full_name\", \"favourite_food\", \"meal_plan\", \"age\"),\n  skip = 1,\n  na = c(\"\", \"N/A\"),\n  col_types = \"dcccc\"\n)\n\nGoogle Sheets differ from Excel in a few ways. The file is identified by a long URL; read_sheet() needs an internet connection; and some functions (e.g., sheet_names(), sheet_write()) behave differently. However, most tasks transfer directly by swapping read_excel() for read_sheet(). If a sheet contains multiple worksheets (tabs), you can specify the sheet argument to read a particular tab or call sheet_names() to list all tabs.\n\n\nKey take‑aways\n\nSpreadsheets are convenient for people but messy for code. Identify the correct worksheet and cell range before importing.\nUse readxl::read_excel() or the specialized read_xls()/read_xlsx() to load Excel files. Specify column names, skip header rows and define missing‑value strings to clean up the import.\nCall excel_sheets() to list worksheets and iterate with purrr::map() when you need to import multiple sheets.\nUse the range argument to read only the rectangle containing the data. Understand that Excel’s underlying data types may differ from their display and adjust col_types accordingly.\nWrite data back to Excel with writexl::write_xlsx(); use openxlsx for more advanced formatting.\nThe googlesheets4 package brings similar functionality to Google Sheets; read_sheet() reads a sheet by URL or ID and accepts the same cleaning arguments.",
    "crumbs": [
      "<span class='chapter-number'>14</span>  <span class='chapter-title'>Session 14 – Advanced data import I: spreadsheets</span>"
    ]
  },
  {
    "objectID": "15.html",
    "href": "15.html",
    "title": "15  Session 15 – Advanced data import II: databases",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 – Advanced data import II: databases</span>"
    ]
  },
  {
    "objectID": "15.html#objectives",
    "href": "15.html#objectives",
    "title": "15  Session 15 – Advanced data import II: databases",
    "section": "",
    "text": "Motivate working directly with databases. Large volumes of data often live in databases instead of flat files, and repeatedly exporting CSV snapshots is time‑consuming. Learn why connecting directly to a database gives you immediate access to current data and avoids delays.\nExplain how database tables differ from tibbles. Database tables live on disk, can be arbitrarily large and usually have indexes to speed up queries. Classical databases prioritize writing over analysis, whereas column‑oriented systems like duckdb are optimized for analytical queries.\nConnect to a database with DBI. The DBI package provides a uniform interface to many DBMSs. You’ll learn to call DBI::dbConnect() with a driver from a package like RPostgres, RMariaDB or duckdb and supply host, port and credentials. We will use duckdb because it runs in process and requires no server.\nLoad and inspect tables. Use dbWriteTable() to copy a tibble into a database, dbListTables() to see what tables exist and dbReadTable() to read a whole table back into R.\nExecute SQL queries via DBI. Write simple SELECT … FROM … WHERE … statements and run them with dbGetQuery(). Understand the core SQL clauses and how they map to dplyr verbs.\nUse dbplyr for lazy pipelines. dbplyr translates dplyr code into SQL. Create a lazy table with tbl(), compose pipelines with verbs like filter() and summarize(), view the generated SQL with show_query() and retrieve results with collect().",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 – Advanced data import II: databases</span>"
    ]
  },
  {
    "objectID": "15.html#notes",
    "href": "15.html#notes",
    "title": "15  Session 15 – Advanced data import II: databases",
    "section": "Notes",
    "text": "Notes\n\nWhy work with databases?\nWhen colleagues give you data by exporting CSV files, you have to ask for a new file every time your questions change. The online textbook warns that this approach soon becomes painful and encourages you to connect directly to the database. A database table looks like a data frame—a collection of named columns—but there are key differences:\n\nStorage: data frames live in memory, whereas database tables live on disk and can be arbitrarily large.\nIndexes: most databases maintain indexes that allow them to locate rows quickly without scanning the whole table.\nOptimization: row‑oriented databases are designed to record transactions quickly, while column‑oriented systems like duckdb are optimized for analytical queries.\n\nThese properties make databases essential for working with large or shared datasets. Rather than extracting snapshots, you can write queries that retrieve exactly the data you need.\n\n\nSetting up a connection\nThe DBI package defines a low‑level interface for connecting to many database management systems. You create a connection with DBI::dbConnect(), supplying a driver (from a DBMS‑specific package) and connection details. For example, to connect to a PostgreSQL server you might write:\n\ncon = DBI::dbConnect(\n  RPostgres::Postgres(),\n  host = \"db.example.com\",\n  port = 5432,\n  dbname = \"mydb\",\n  user = \"username\",\n  password = \"secret\"\n)\n\nIn this course we will use duckdb, an in‑process database that lives entirely within R. It is designed for data scientists and is easy to get started with:\n\nlibrary(DBI)\nlibrary(duckdb)\n\n# Create a temporary duckdb database in memory\ncon = dbConnect(duckdb())\n\n# Disconnect and clean up when finished\ndbDisconnect(con, shutdown = TRUE)\n\nIf you want to keep data between sessions, provide the dbdir argument to specify a file on disk.\n\n\nLoading and inspecting tables\nNew databases are empty, so you need to load data before you can query it. Use dbWriteTable() to copy a tibble into a database; the first argument is the connection, the second is a table name and the third is the data frame. For example, to load the mpg dataset from ggplot2 into your duckdb database:\n\nlibrary(ggplot2)\n\ndbWriteTable(con, \"mpg\", mpg)\n\nYou can explore your database with dbListTables() and read an entire table back into R with dbReadTable():\n\n# List tables\ndbListTables(con)\n\n[1] \"mpg\"\n\n# Read a table\ndbReadTable(con, \"mpg\") |&gt; tibble::as_tibble()\n\n# A tibble: 234 × 11\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ 224 more rows\n\n\n\n\nRunning SQL queries\nIf you know SQL, you can execute queries with DBI::dbGetQuery(). A SQL query consists of clauses such as SELECT, FROM, WHERE, GROUP BY and ORDER BY. For instance, to find cars with highway mileage above 30 in the mpg table:\n\nsql = \"SELECT manufacturer, model, year, hwy\n        FROM mpg\n        WHERE hwy &gt; 30\n        ORDER BY hwy DESC\"\nfast_cars = dbGetQuery(con, sql)\nhead(fast_cars)\n\nSQL gives you full control over the query, but writing it can be verbose. In many cases you can use dbplyr to write dplyr code instead.\n\n\nLazy queries with dbplyr\nThe dbplyr package translates your dplyr code into SQL for you. Start by creating a lazy table with tbl(); it represents a database table but doesn’t pull any data into R:\n\nlibrary(dplyr)\n\nmpg_db = tbl(con, \"mpg\")\nmpg_db\n\n# Source:   table&lt;mpg&gt; [?? x 11]\n# Database: DuckDB 1.4.1 [Joshua_Patrick@Windows 10 x64:R 4.5.1/:memory:]\n   manufacturer model      displ  year   cyl trans drv     cty   hwy fl    class\n   &lt;chr&gt;        &lt;chr&gt;      &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt;\n 1 audi         a4           1.8  1999     4 auto… f        18    29 p     comp…\n 2 audi         a4           1.8  1999     4 manu… f        21    29 p     comp…\n 3 audi         a4           2    2008     4 manu… f        20    31 p     comp…\n 4 audi         a4           2    2008     4 auto… f        21    30 p     comp…\n 5 audi         a4           2.8  1999     6 auto… f        16    26 p     comp…\n 6 audi         a4           2.8  1999     6 manu… f        18    26 p     comp…\n 7 audi         a4           3.1  2008     6 auto… f        18    27 p     comp…\n 8 audi         a4 quattro   1.8  1999     4 manu… 4        18    26 p     comp…\n 9 audi         a4 quattro   1.8  1999     4 auto… 4        16    25 p     comp…\n10 audi         a4 quattro   2    2008     4 manu… 4        20    28 p     comp…\n# ℹ more rows\n\n\nYou can now use familiar dplyr verbs to filter, group and summarize. dbplyr records the operations and only runs the query when you call collect(). For example, to compute the average highway mileage for cars with at least a 2.0 dipslacement:\n\n# Compose a lazy query\nsummary_db = mpg_db |&gt;\n  filter(displ &gt;= 2.0) |&gt;\n  group_by(manufacturer) |&gt;\n  summarise(avg_hwy = mean(hwy), n = n()) |&gt;\n  arrange(desc(avg_hwy))\n\n# Show the generated SQL\nsummary_db |&gt; show_query()\n\n&lt;SQL&gt;\nSELECT manufacturer, AVG(hwy) AS avg_hwy, COUNT(*) AS n\nFROM (\n  SELECT mpg.*\n  FROM mpg\n  WHERE (displ &gt;= 2.0)\n) q01\nGROUP BY manufacturer\nORDER BY avg_hwy DESC\n\n# Retrieve the results\nsummary = summary_db |&gt; collect()\nsummary\n\n# A tibble: 15 × 3\n   manufacturer avg_hwy     n\n   &lt;chr&gt;          &lt;dbl&gt; &lt;dbl&gt;\n 1 honda           29       1\n 2 volkswagen      27.4    22\n 3 hyundai         26.9    14\n 4 pontiac         26.4     5\n 5 audi            26.2    14\n 6 subaru          25.6    14\n 7 nissan          24.6    13\n 8 toyota          23.3    29\n 9 chevrolet       21.9    19\n10 ford            19.4    25\n11 mercury         18       4\n12 dodge           17.9    37\n13 jeep            17.6     8\n14 lincoln         17       3\n15 land rover      16.5     4\n\n\nThe show_query() function prints the SQL generated by dbplyr. When you call collect(), dbplyr sends the query to the database, retrieves the results and converts them into a tibble.\n\n\nBasic SQL structure\nAlthough dbplyr hides much of SQL, it’s useful to understand its structure. A query typically includes the following clauses:\n\nSELECT specifies which columns to return and can compute new columns.\nFROM names the table being queried.\nWHERE filters rows based on a condition.\nGROUP BY summarises groups of rows.\nORDER BY sorts the output.\n\nSQL keywords are case‑insensitive, but it’s common to write them in uppercase. The clauses must appear in the order shown.",
    "crumbs": [
      "<span class='chapter-number'>15</span>  <span class='chapter-title'>Session 15 – Advanced data import II: databases</span>"
    ]
  },
  {
    "objectID": "16.html",
    "href": "16.html",
    "title": "16  Session 16 – Big‑data formats & hierarchical data",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16 – Big‑data formats & hierarchical data</span>"
    ]
  },
  {
    "objectID": "16.html#objectives",
    "href": "16.html#objectives",
    "title": "16  Session 16 – Big‑data formats & hierarchical data",
    "section": "",
    "text": "Understand why CSVs aren’t enough. Recall that CSV files are easy to read but inefficient: you have to do a lot of work to read the data, and they don’t scale well to tens of millions of rows. Learn about the parquet format, an open, column‑oriented file format widely used by big‑data systems. Appreciate why parquet files are more efficient than CSVs and how they enable faster reads and writes.\nUse Apache Arrow for larger‑than‑memory data. The arrow project is a multi‑language toolbox designed for efficient analysis and transport of large datasets. The arrow R package provides a dplyr backend so you can analyse larger‑than‑memory datasets using familiar syntax. Learn how to open datasets lazily with open_dataset() and work with them without reading everything into memory.\nDescribe lists and hierarchical data. Hierarchical or tree‑like data structures are common, especially when working with web APIs or JSON. A list is a vector that can store elements of different types; lists underpin hierarchical data. Understand the difference between atomic vectors and lists, and recognize list‑columns inside tibbles.\nRectangling with tidyr. Data rectangling converts hierarchical data into tidy rectangular tables. Learn to unnest list‑columns with unnest_longer() (which repeats rows) and unnest_wider() (which spreads components across columns). Appreciate when each is appropriate and how to handle inconsistent types.\nPrepare for web scraping. Hierarchical data often comes from the web. Being comfortable with lists and unnesting will prepare you for the next session on scraping HTML pages.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16 – Big‑data formats & hierarchical data</span>"
    ]
  },
  {
    "objectID": "16.html#notes",
    "href": "16.html#notes",
    "title": "16  Session 16 – Big‑data formats & hierarchical data",
    "section": "Notes",
    "text": "Notes\n\nWhy consider big‑data formats?\nCSV files are simple and human‑readable: they can be read by almost every tool under the sun. However, simplicity comes at a cost. Reading a large CSV into R requires scanning every character and converting each column to the correct type. If you have a dataset with tens of millions of rows, this process is slow and memory‑hungry. In the Arrow chapter of R for Data Science, the authors introduce the parquet format as a powerful alternative. Parquet is a column‑oriented, compressed, open standard that is widely used by big‑data systems. Because it stores data column‑by‑column, parquet allows you to read only the columns you need and skip over the rest, leading to much faster IO and smaller file sizes. When your dataset lives in parquet files, you can analyse it directly without converting it to CSV.\n\n\nApache Arrow and the arrow package\nWorking with large datasets requires more than a good file format. Apache Arrow is a multi‑language toolkit for efficient in‑memory columnar data representation. The arrow package for R wraps this toolkit and provides a dplyr backend that looks and feels like working with a normal tibble. Instead of reading a parquet or CSV file into memory, you call open_dataset(). This function scans just enough of the file(s) to determine the schema (column names and types) and then lazily reads data as you request it. Arrow datasets can span multiple files on disk and handle data sets larger than your computer’s memory.\nHere is a small demonstration using the starwars dataset (from the dplyr package). While the dataset itself is small, the workflow scales to larger data. We’ll write it to a parquet file, open it with arrow, and compute a summary. Note that the arrow package must be installed.\n\nlibrary(tidyverse)\nlibrary(arrow)\n\n# Write the built‑in starwars dataset to a parquet file\nwrite_parquet(starwars, \"starwars.parquet\")\n\n# Open the parquet file lazily\nsw_ds = open_dataset(\"starwars.parquet\")\n\n# Compute the average height by species without loading all rows into memory\navg_height = sw_ds |&gt;\n  group_by(species) |&gt;\n  summarise(n = n(), avg_height = mean(height, na.rm = TRUE), .groups = \"drop\") |&gt;\n  arrange(desc(avg_height)) |&gt;\n  collect()\n\nhead(avg_height)\n\n# A tibble: 6 × 3\n  species      n avg_height\n  &lt;chr&gt;    &lt;int&gt;      &lt;dbl&gt;\n1 Quermian     1       264 \n2 Wookiee      2       231 \n3 Kaminoan     2       221 \n4 Kaleesh      1       216 \n5 Gungan       3       209.\n6 Pau'an       1       206 \n\n\nIn this example, open_dataset() returns an object representing the on‑disk dataset. The group_by() and summarise() calls are recorded but not executed until you call collect(), at which point Arrow reads only the necessary columns from disk and computes the summary. This approach allows you to analyze datasets much larger than memory using the same dplyr verbs you’ve learned.\n\n\nLists and hierarchical data\nThe second part of this session focuses on hierarchical data, which arises when observations are nested inside one another. Such data are common when working with web APIs, JSON files, or documents that naturally have tree structures. To work with hierarchical data in R, you need to understand lists. A list is a vector whose elements can be of different types. You create a list with list(), and you can name its components just like naming columns of a tibble. For example:\n\nx1 = list(1:4, \"a string\", TRUE)     # unnamed list\nx1\n\n[[1]]\n[1] 1 2 3 4\n\n[[2]]\n[1] \"a string\"\n\n[[3]]\n[1] TRUE\n\nx2 = list(numbers = 1:3, letters = letters[1:3], flag = FALSE)  # named list\nx2\n\n$numbers\n[1] 1 2 3\n\n$letters\n[1] \"a\" \"b\" \"c\"\n\n$flag\n[1] FALSE\n\n\nLists become especially interesting inside a data frame as list‑columns. The starwars tibble contains several list‑columns: films, vehicles and starships are character vectors inside each row. These columns store multiple values per observation, so they are inherently hierarchical. Inspecting starwars reveals the list columns:\n\nglimpse(starwars)\n\nRows: 87\nColumns: 14\n$ name       &lt;chr&gt; \"Luke Skywalker\", \"C-3PO\", \"R2-D2\", \"Darth Vader\", \"Leia Or…\n$ height     &lt;int&gt; 172, 167, 96, 202, 150, 178, 165, 97, 183, 182, 188, 180, 2…\n$ mass       &lt;dbl&gt; 77.0, 75.0, 32.0, 136.0, 49.0, 120.0, 75.0, 32.0, 84.0, 77.…\n$ hair_color &lt;chr&gt; \"blond\", NA, NA, \"none\", \"brown\", \"brown, grey\", \"brown\", N…\n$ skin_color &lt;chr&gt; \"fair\", \"gold\", \"white, blue\", \"white\", \"light\", \"light\", \"…\n$ eye_color  &lt;chr&gt; \"blue\", \"yellow\", \"red\", \"yellow\", \"brown\", \"blue\", \"blue\",…\n$ birth_year &lt;dbl&gt; 19.0, 112.0, 33.0, 41.9, 19.0, 52.0, 47.0, NA, 24.0, 57.0, …\n$ sex        &lt;chr&gt; \"male\", \"none\", \"none\", \"male\", \"female\", \"male\", \"female\",…\n$ gender     &lt;chr&gt; \"masculine\", \"masculine\", \"masculine\", \"masculine\", \"femini…\n$ homeworld  &lt;chr&gt; \"Tatooine\", \"Tatooine\", \"Naboo\", \"Tatooine\", \"Alderaan\", \"T…\n$ species    &lt;chr&gt; \"Human\", \"Droid\", \"Droid\", \"Human\", \"Human\", \"Human\", \"Huma…\n$ films      &lt;list&gt; &lt;\"A New Hope\", \"The Empire Strikes Back\", \"Return of the J…\n$ vehicles   &lt;list&gt; &lt;\"Snowspeeder\", \"Imperial Speeder Bike\"&gt;, &lt;&gt;, &lt;&gt;, &lt;&gt;, \"Imp…\n$ starships  &lt;list&gt; &lt;\"X-wing\", \"Imperial shuttle\"&gt;, &lt;&gt;, &lt;&gt;, \"TIE Advanced x1\",…\n\n\nEach row represents a character, but the films column is itself a character vector of film titles. To analyse such data, we need to rectangulate it: convert it into a tidy data frame with one value per cell.\n\n\nUnnesting with unnest_longer() and unnest_wider()\nThe tidyr package provides two main functions for rectangling list‑columns: unnest_longer() and unnest_wider().\n\nunnest_longer() takes each element of a list‑column and puts it on its own row, repeating the other columns as needed. Use it when each element of the list is a vector of homogeneous type (e.g., a character vector of films).\n\nunnest_wider() takes a list of named objects (e.g., a list of tibbles) and spreads its components across new columns. Use it when each element of the list is itself a record with multiple fields.\n\nLet’s use unnest_longer() to count how many characters appear in each Star Wars film:\n\nlibrary(tidyr)\n\n# Unnest the films list‑column\ncharacters_by_film = starwars |&gt;\n  select(name, films) |&gt;\n  unnest_longer(films) |&gt;\n  count(films, name = \"n_characters\") |&gt;\n  arrange(desc(n_characters))\n\ncharacters_by_film\n\n# A tibble: 7 × 2\n  films                   n_characters\n  &lt;chr&gt;                          &lt;int&gt;\n1 Attack of the Clones              40\n2 Revenge of the Sith               34\n3 The Phantom Menace                34\n4 Return of the Jedi                20\n5 A New Hope                        18\n6 The Empire Strikes Back           16\n7 The Force Awakens                 11\n\n\nHere unnest_longer(films) converts each character’s list of film titles into individual rows, then count() tallies the number of characters per film. Had the list‑column contained named lists or data frames, you could instead use unnest_wider() to turn the nested fields into separate columns.\n\n\nJSON and other hierarchical sources\nMuch hierarchical data comes from the web in JSON format. JSON is a text format for representing nested objects and arrays. You can read JSON into R with the jsonlite package, which produces nested lists. Once in R, use the same rectangling tools (unnest_longer(), unnest_wider()) to convert nested lists into tidy tibbles. In the next session on web scraping you’ll encounter HTML, another hierarchical structure. A strong grasp of lists and unnesting will enable you to extract structured data from these sources.\n\n\nKey take‑aways\n\nCSV vs. parquet: CSVs are simple but inefficient. Parquet files store data column‑by‑column and compress it, making them faster to read and write for big data.\nArrow for bigger data: The arrow package lets you open large datasets lazily with open_dataset() and analyse them using dplyr syntax. The data live on disk and are only read when needed.\nLists underpin hierarchical data: A list can hold elements of different types. List‑columns in tibbles allow you to store multiple values per observation.\nRectangling tools: Use tidyr::unnest_longer() to repeat rows for each element of a list‑column and unnest_wider() to spread named components across columns.\nPreparation for scraping: Understanding lists and rectangling prepares you to handle data from JSON and web pages, which you’ll encounter in the next session on web scraping.",
    "crumbs": [
      "<span class='chapter-number'>16</span>  <span class='chapter-title'>Session 16 – Big‑data formats & hierarchical data</span>"
    ]
  },
  {
    "objectID": "17.html",
    "href": "17.html",
    "title": "17  Session 17 – Web scraping",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 – Web scraping</span>"
    ]
  },
  {
    "objectID": "17.html#objectives",
    "href": "17.html#objectives",
    "title": "17  Session 17 – Web scraping",
    "section": "",
    "text": "Understand the ethics and legalities of web scraping. Scraping data can raise legal and ethical questions. You’ll learn to assess whether data are public, non‑personal and factual, respect terms of service and avoid scraping sensitive personal information. Use tools like the polite package to space out your requests.\nUnderstand the structure of HTML. Web pages are hierarchical documents written in HTML. Each element consists of a start tag, optional attributes and an end tag. Recognize block tags (e.g., &lt;p&gt;, &lt;section&gt;) and inline tags (e.g., &lt;a&gt;, &lt;b&gt;).\nLearn basic CSS selectors. CSS selectors define patterns for locating HTML elements. You’ll see how simple selectors like p, .class and #id identify elements by tag, class or id.\nUse rvest to extract data. The rvest package provides functions to find elements (html_elements() and html_element()), extract text (html_text2()), extract attributes (html_attr()) and convert HTML tables to tibbles (html_table()). Understand the difference between html_elements() and html_element() and when to use each.\nFind the right selector. Selecting the right elements often requires trial and error. Learn to use SelectorGadget and browser developer tools to identify CSS selectors and copy them.\nPrepare for dynamic sites. Recognize that some websites load content via JavaScript, which rvest alone cannot handle. Understand alternatives like using APIs when available.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 – Web scraping</span>"
    ]
  },
  {
    "objectID": "17.html#notes",
    "href": "17.html#notes",
    "title": "17  Session 17 – Web scraping",
    "section": "Notes",
    "text": "Notes\n\nEthics and legalities\nBefore you scrape a website, ask whether it’s legal and ethical. Legalities vary by jurisdiction, but a good rule of thumb is that if the data are public, non‑personal and factual, you’re usually okay. Avoid scraping proprietary or sensitive data or anything behind a login. Even when data are public, respect the website’s terms of service (often in the footer) and any robots.txt directives; although these terms may not always be enforceable, it’s polite to follow them. Never scrape personally identifiable information such as names, email addresses or dates of birth; scraping such data can violate privacy laws. Additionally, respect the website’s resources: spread out your requests and avoid overloading the server. The polite package automates pauses between requests and caches pages to avoid repeated downloads.\n\n\nHTML basics\nWeb pages are built with HyperText Markup Language (HTML). An HTML document consists of nested elements, each with a start tag, optional attributes, an end tag and contents. The &lt;html&gt; element contains two main children: &lt;head&gt;, which holds metadata, and &lt;body&gt;, which holds the content you see in the browser. Elements like &lt;h1&gt;, &lt;p&gt; and &lt;section&gt; structure the document, while inline elements like &lt;b&gt; and &lt;a&gt; format text. HTML uses entity escapes such as &gt; and &lt; for &lt; and &gt;; however, rvest automatically handles these escapes. Because HTML is hierarchical, the data you want to scrape is often nested within several layers of tags.\n\n\nCSS selectors\nTo extract information from HTML, you need to identify which elements contain your data. CSS selectors provide a concise language for selecting elements on a page. Basic selectors include:\n\np – selects all &lt;p&gt; elements.\n.title – selects elements whose class attribute is “title”.\n#title – selects the element with id=\"title\".\n\nYou can combine selectors with spaces to nest selections. For example, ul li selects all &lt;li&gt; elements inside a &lt;ul&gt;. CSS selectors are case‑sensitive for classes and ids.\n\n\nExtracting data with rvest\nThe rvest package makes it straightforward to scrape HTML. The typical workflow is:\n\nDownload the page with read_html(url), which returns an XML document.\nFind elements with html_elements(doc, \"selector\") to return all matches or html_element(doc, \"selector\") to return the first match. Use the singular form when you need exactly one element per observation.\nExtract data:\n\nUse html_text2() to extract the textual content of an element. The html_text2() function strips whitespace and handles HTML escapes.\nUse html_attr(element, \"href\") or another attribute name to extract the value of an attribute.\nTo extract tables, use html_table() on a &lt;table&gt; element. It returns a list of tibbles; each tibble corresponds to one table on the page.\n\nTidy the result using dplyr and tidyr.\n\nHere is a minimal example that demonstrates the difference between html_elements() and html_element(). Suppose html is a list of &lt;li&gt; elements; html_elements(html, \".weight\") returns all weight spans, which may not align with the number of &lt;li&gt;s, whereas html_element(html, \".weight\") returns one match per &lt;li&gt;, returning NA if an element is missing.\nWhen extracting tables, rvest automatically converts numeric columns to numbers, but conversion isn’t perfect. You can disable automatic conversion by passing convert = FALSE to html_table() and then use readr or dplyr functions to convert columns.\n\n\nFinding selectors\nChoosing the right selector often requires trial and error. Use browser developer tools (e.g., Chrome DevTools) to inspect the HTML structure: right‑click on the element of interest and choose Inspect. You can then copy a unique selector or identify classes and ids to build your own. SelectorGadget is a bookmarklet that helps generate selectors by clicking on positive and negative examples. If the selectors are hard to understand, consult the CSS Dinner game or MDN Web Docs for a friendly introduction to CSS selectors.\n\n\nWorkflow and dynamic sites\nA typical scraping workflow combines these tools:\n\nLoad the page and identify candidate elements using developer tools.\nStart with simple selectors and refine them until you extract the desired elements.\nUse html_text2(), html_attr() and html_table() to convert the elements into R data structures.\nClean and tidy the data; convert strings to numbers, handle missing values and join tables as needed.\nBe respectful: add pauses (Sys.sleep()) between requests or use polite.\n\nBe aware that some websites render their content with JavaScript. rvest only downloads the static HTML; it cannot run JavaScript. If the data you need isn’t present in the downloaded HTML, consider looking for an API or using tools like RSelenium. Whenever possible, prefer APIs because they provide stable, structured data.\n\n\nKey take‑aways\n\nEthics first: only scrape public, non‑personal, factual data and obey terms of service.\nHTML is hierarchical: understand tags, attributes and nesting to find your data.\nCSS selectors are powerful: simple selectors like tag, class and id get you far.\nrvest functions: use html_elements()/html_element() to find elements, html_text2() and html_attr() to extract text and attributes, and html_table() to read HTML tables.\nIterative process: expect to iterate when choosing selectors; use tools like SelectorGadget and developer tools.",
    "crumbs": [
      "<span class='chapter-number'>17</span>  <span class='chapter-title'>Session 17 – Web scraping</span>"
    ]
  },
  {
    "objectID": "18.html",
    "href": "18.html",
    "title": "18  Session 18 – Basic programming I: functions & code style",
    "section": "",
    "text": "Objectives",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 – Basic programming I: functions & code style</span>"
    ]
  },
  {
    "objectID": "18.html#objectives",
    "href": "18.html#objectives",
    "title": "18  Session 18 – Basic programming I: functions & code style",
    "section": "",
    "text": "Recognize why writing functions matters. Functions let you automate common tasks and greatly extend your reach as a data scientist. Writing a function has four big advantages over copy‑and‑paste: you can give your code an evocative name, you only need to update code in one place as requirements change, you avoid copy‑and‑paste mistakes, and you can easily reuse your work on future projects. A good rule of thumb is to write a function whenever you’ve copied and pasted the same block of code more than twice.\nDifferentiate types of functions. Learn the three broad categories of functions: vector functions that take one or more vectors and return a vector, data‑frame functions that take a data frame as input and return a data frame or vector, and plot functions that take a data frame and return a plot.\nConstruct functions from repeated code. Identify the parts of your code that vary and those that stay constant; choose a short, descriptive name, list the varying pieces as arguments and wrap the repeated body of code in function(). Test your function on simple inputs before using it in pipelines.\nUnderstand tidy evaluation and embracing. When you write data‑frame functions that call dplyr verbs, you encounter the problem of indirection: passing column names as arguments doesn’t work because dplyr uses tidy evaluation. Use the embracing operator { } to tell dplyr to look inside the argument for the variable to use. Recognize which arguments require embracing by checking whether the underlying verb uses data‑masking (computations like filter() and summarize()) or tidy‑selection (selectors like select() and rename()).\nWrite simple plot functions. Plot functions wrap ggplot2 code into reusable helpers. Because aes() is a data‑masking function you can embrace variables passed to your function so the user can specify which column to plot.\nAdopt good naming and style conventions. Choose function names that are verbs and argument names that are nouns; clarity is more important than brevity. Put the body of your function inside curly braces with proper indentation and add spaces inside { } to make embracing obvious.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 – Basic programming I: functions & code style</span>"
    ]
  },
  {
    "objectID": "18.html#notes",
    "href": "18.html#notes",
    "title": "18  Session 18 – Basic programming I: functions & code style",
    "section": "Notes",
    "text": "Notes\n\nWhy write functions?\nOne of the most effective ways to reduce duplication and avoid errors is to move repeated code into a function. R for Data Science argues that writing a function has four important benefits: you can give the code a clear name, centralize changes in one place, eliminate copy‑and‑paste mistakes and reuse work across projects. Whenever you find yourself copying and pasting the same code more than twice, it’s time to consider writing a function.\n\n\nAnatomy of a function\nTo convert repeated code into a function, identify what changes and what stays the same. A function needs three things: a name, a set of arguments and a body that contains the repeated code. For example, suppose you need to rescale a numeric vector to lie between 0 and 1. You can encapsulate this pattern in a function called rescale01():\n\nrescale01 = function(x) {\n  rng = range(x, na.rm = TRUE, finite = TRUE)\n  (x - rng[1]) / (rng[2] - rng[1])\n}\n\n# test it\nrescale01(c(-10, 0, 10))\n\n[1] 0.0 0.5 1.0\n\n\nThe example illustrates the basic template name = function(arguments) { body }. By encapsulating the code, you only need to modify the function in one place if you decide to handle infinite values differently.\n\n\nVector functions\nA vector function takes one or more vectors as input and returns a vector of the same length. Common examples include rescaling or transforming variables. You can use vector functions inside mutate() or other dplyr verbs. For instance, a z‑score function:\n\nz_score = function(x) {\n  (x - mean(x, na.rm = TRUE)) / sd(x, na.rm = TRUE)\n}\n\nBecause z_score() returns a vector, you can apply it to multiple columns in one step using across() (covered in the next session).\n\n\nData‑frame functions and tidy evaluation\nData‑frame functions take a data frame as input, perform some operation and return a new data frame or summary. The challenge is that dplyr verbs use tidy evaluation: they interpret bare variable names within the context of the data. If you write a function like:\n\nlibrary(tidyverse)\n\ngrouped_mean = function(df, group_var, mean_var) {\n  df |&gt;\n    group_by(group_var) |&gt;\n    summarize(mean(mean_var))\n}\n\nit won’t work because group_by() and summarize() look for columns literally named group_var and mean_var. To solve this problem, embrace your arguments with { }, which tells dplyr to use the value of the argument rather than its name:\n\ngrouped_mean = function(df, group_var, mean_var) {\n  df |&gt;\n    group_by({{ group_var }}) |&gt;\n    summarize(mean = mean({{ mean_var }}, na.rm = TRUE), .groups = \"drop\")\n}\n\n# use it\ndiamonds |&gt;\n  grouped_mean(cut, carat)\n\n# A tibble: 5 × 2\n  cut        mean\n  &lt;ord&gt;     &lt;dbl&gt;\n1 Fair      1.05 \n2 Good      0.849\n3 Very Good 0.806\n4 Premium   0.892\n5 Ideal     0.703\n\n\nEmbracing works for both data‑masking arguments (e.g., in filter(), summarize() or arrange()) and tidy‑selection arguments (e.g., in select() or rename()). When in doubt, consult the documentation to see whether an argument uses tidy evaluation and needs embracing.\nAnother useful pattern is to wrap up a common set of summaries into a helper function. The following summary6() computes the minimum, mean, median and maximum of a variable along with the number of observations and missing values:\n\nsummary6 = function(data, var) {\n  data |&gt;\n    summarize(\n      min   = min({{ var }}, na.rm = TRUE),\n      mean  = mean({{ var }}, na.rm = TRUE),\n      median= median({{ var }}, na.rm = TRUE),\n      max   = max({{ var }}, na.rm = TRUE),\n      n     = n(),\n      n_miss= sum(is.na({{ var }})),\n      .groups = \"drop\"\n    )\n}\n\nBecause it wraps summarize(), you can apply summary6() to grouped data or use it inside other functions. Always drop grouping (.groups = \"drop\") inside helpers to avoid surprises.\n\n\nPlot functions\nYou can also write functions that return plots. For example, repeated calls to ggplot() and geom_histogram() can be wrapped into a histogram() helper:\n\nhistogram = function(df, var, binwidth = NULL) {\n  df |&gt;\n    ggplot(aes(x = {{ var }})) +\n    geom_histogram(binwidth = binwidth)\n}\n\n# example\ndiamonds |&gt;\n  histogram(carat, binwidth = 0.1)\n\n\n\n\n\n\n\n\nThe key is that aes() is a data‑masking function, so you can embrace the variable passed to the function. Users can still add layers to the returned plot with +. You can write other plot helpers, such as a linearity_check() that overlays a smooth curve and straight line to diagnose linear relationships.\n\n\nFunction style\nGood naming and formatting make your functions readable for future you and others. R doesn’t care about names or white space, but humans do. Prefer verbs for function names and nouns for arguments; clarity beats brevity. For example, impute_missing() and collapse_years() are more informative than f() or my_awesome_function(). Always follow function() with curly braces and indent the body by two spaces. When embracing variables, put spaces inside { } to make it obvious that something special is happening.\n\n\nKey take‑aways\n\nFunctions reduce duplication and errors. When you see repeated code, bundle it into a function. Functions centralize changes, reduce mistakes and make your code more readable.\nIdentify what varies. Analyse your code to determine which pieces vary and make those arguments; the rest becomes the body of the function.\nUse embracing with dplyr verbs. Tidy evaluation means that column names inside dplyr verbs are treated differently. When writing data‑frame functions, use { } around arguments that refer to column names.\nWrap up summaries and plots. Helper functions like summary6() or histogram() reduce copy‑and‑paste when you need the same summaries or plots repeatedly.\nName and style your functions thoughtfully. Use descriptive verbs, nouns for arguments, proper indentation and spaces in braces to make your code easy to read.",
    "crumbs": [
      "<span class='chapter-number'>18</span>  <span class='chapter-title'>Session 18 – Basic programming I: functions & code style</span>"
    ]
  }
]